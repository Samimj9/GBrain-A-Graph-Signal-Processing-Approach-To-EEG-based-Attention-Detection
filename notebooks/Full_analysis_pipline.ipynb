{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811d125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 1. Import Necessary Libraries\n",
    "\n",
    "#%%\n",
    "import mne\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, RocCurveDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import ScalarMappable\n",
    "from matplotlib.colors import Normalize\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3ec5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. Define your base EEG data folder\n",
    "#    This folder now directly contains your .fif files\n",
    "base_eeg_folder = \"C:\\\\Users\\\\LENOVO\\\\OneDrive - City University\\\\Desktop\\\\Attention detection FYP\\\\Data\"\n",
    "\n",
    "# 2. Define the LIST of subject IDs you want to load data for\n",
    "subject_ids = ['S02','S03','S04', 'S05','S06','S07','S08','S09','S10','S11'] # Add more subjects as needed, e.g., ['S01', 'S02', 'S03', ...]\n",
    "# artifact_subject_ids = ['V01', 'V02', 'V03', 'V04', 'V05', 'V06'] # List of subjects with artifacts\n",
    "# --- Dictionary to store raw data and epochs for each subject ---\n",
    "raw_data_by_subject = {}\n",
    "epochs_by_subject = {}\n",
    "\n",
    "# Select only the specified channels (this remains constant for all subjects)\n",
    "channels = ['Fp1', 'Fp2', 'Fz', 'F3', 'F4', 'C3', 'Cz', 'C4']\n",
    "\n",
    "# --- Loop through each subject ID to load and preprocess their data ---\n",
    "print(f\"--- Starting data loading for subjects: {subject_ids} ---\")\n",
    "for current_subject_id in subject_ids:\n",
    "    print(f\"\\nProcessing data for {current_subject_id}...\")\n",
    "\n",
    "    # --- CORRECTED PATH CONSTRUCTION ---\n",
    "    # Now, we directly join base_eeg_folder with the filename,\n",
    "    # as there's no subfolder for the subject ID.\n",
    "    attention_file = os.path.join(base_eeg_folder, f\"Att_{current_subject_id}_cleaned_raw.fif\")\n",
    "    inattention_file = os.path.join(base_eeg_folder, f\"Inatt_{current_subject_id}_cleaned_raw.fif\")\n",
    "    # IMPORTANT: Double-check 'Inatt' capitalization for S05 if needed, you wrote 'inatt_S05...'\n",
    "    # If the file is actually 'inatt_S05_cleaned_raw3.fif' (lowercase 'i'), then change the line above to:\n",
    "    # inattention_file = os.path.join(base_eeg_folder, f\"inatt_{current_subject_id}_cleaned_raw3.fif\")\n",
    "\n",
    "\n",
    "    # --- Verify paths (optional, but good for debugging) ---\n",
    "    print(f\"  Attempting to load Attention file: {attention_file}\")\n",
    "    print(f\"  Attempting to load Inattention file: {inattention_file}\")\n",
    "\n",
    "    # --- Load the raw FIF files ---\n",
    "    try:\n",
    "        raw_att = mne.io.read_raw_fif(attention_file, preload=True, verbose=False)\n",
    "        raw_inatt = mne.io.read_raw_fif(inattention_file, preload=True, verbose=False)\n",
    "        print(f\"  Raw Attention data shape: {raw_att.get_data().shape} (channels x time points)\")\n",
    "        print(f\"  Raw Inattention data shape: {raw_inatt.get_data().shape} (channels x time points)\")\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"  ERROR: File not found for {current_subject_id}. Skipping this subject. Details: {e}\")\n",
    "        continue # Skip to the next subject if files are missing\n",
    "\n",
    "    # Select only the specified channels\n",
    "    raw_att.pick_channels(channels, verbose=False)\n",
    "    raw_inatt.pick_channels(channels, verbose=False)\n",
    "    print(f\"  Raw Attention data shape after channel selection: {raw_att.get_data().shape}\")\n",
    "    print(f\"  Raw Inattention data shape after channel selection: {raw_inatt.get_data().shape}\")\n",
    "\n",
    "    # Store raw data objects\n",
    "    raw_data_by_subject[current_subject_id] = {'raw_att': raw_att, 'raw_inatt': raw_inatt}\n",
    "\n",
    "    # --- Epoching ---\n",
    "    epochs_att = mne.make_fixed_length_epochs(raw_att, duration=10.0, overlap=5, preload=True, verbose=False)\n",
    "    epochs_inatt = mne.make_fixed_length_epochs(raw_inatt, duration=10.0, overlap=5, preload=True, verbose=False)\n",
    "\n",
    "    print(f\"  {current_subject_id} Attention epochs: {len(epochs_att)} epochs, data shape: {epochs_att.get_data().shape} (epochs x channels x time points)\")\n",
    "    print(f\"  {current_subject_id} Inattention epochs: {len(epochs_inatt)} epochs, data shape: {epochs_inatt.get_data().shape} (epochs x channels x time points)\")\n",
    "\n",
    "    # Store epoch objects\n",
    "    epochs_by_subject[current_subject_id] = {'epochs_att': epochs_att, 'epochs_inatt': epochs_inatt}\n",
    "\n",
    "print(\"\\n--- Finished loading and epoching data for all specified subjects ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b2c949",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne_connectivity import spectral_connectivity_epochs\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from joblib import Parallel, delayed # For parallelizing MI computation\n",
    "from scipy.stats import pearsonr # Import pearsonr for Pearson's correlation\n",
    "\n",
    "# --- Connectivity Computation Functions ---\n",
    "\n",
    "def compute_spectral_connectivity_per_epoch(epochs, method, fmin=4, fmax=35):\n",
    "    \"\"\"\n",
    "    Computes spectral connectivity (Coherence or DPLI) for each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : mne.Epochs\n",
    "        The MNE Epochs object containing the EEG data.\n",
    "    method : str\n",
    "        The connectivity method to compute ('coh' for Coherence, 'dpli' for DPLI).\n",
    "    fmin : int\n",
    "        Minimum frequency of the band of interest.\n",
    "    fmax : int\n",
    "        Maximum frequency of the band of interest.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of connectivity matrices, shape (n_epochs, n_channels, n_channels).\n",
    "    \"\"\"\n",
    "    n_epochs = len(epochs)\n",
    "    sfreq = epochs.info['sfreq']\n",
    "    n_channels = len(epochs.ch_names)\n",
    "\n",
    "    all_connectivity_matrices = []\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        single_epoch = epochs[i:i+1]\n",
    "\n",
    "        con = spectral_connectivity_epochs(\n",
    "            single_epoch,\n",
    "            method=method,\n",
    "            mode='multitaper',\n",
    "            sfreq=sfreq,\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "            faverage=True,\n",
    "            verbose=False,\n",
    "            n_jobs=1 # MNE's n_jobs for this specific function\n",
    "        )\n",
    "\n",
    "        data = con.get_data(output='dense')\n",
    "\n",
    "        if data.ndim == 3:\n",
    "            connectivity_matrix = data[:, :, 0]\n",
    "        else:\n",
    "            connectivity_matrix = data\n",
    "\n",
    "        # Apply corrections based on the connectivity method\n",
    "        if method == 'coh':\n",
    "            # Coherence matrices should be symmetric and have 1.0 on the diagonal.\n",
    "            connectivity_matrix = np.maximum(connectivity_matrix, connectivity_matrix.T)\n",
    "            np.fill_diagonal(connectivity_matrix, 1.0)\n",
    "        elif method == 'dpli':\n",
    "            # DPLI is based on phase lag, so diagonal should be 0.\n",
    "            # DPLI can be slightly asymmetric due to implementation, but typically treated as undirected for graph analysis.\n",
    "            # Forcing symmetry by taking the maximum ensures a consistent undirected graph representation.\n",
    "            connectivity_matrix = np.maximum(connectivity_matrix, connectivity_matrix.T)\n",
    "            np.fill_diagonal(connectivity_matrix, 0.0)\n",
    "\n",
    "        all_connectivity_matrices.append(connectivity_matrix)\n",
    "\n",
    "    all_connectivity_matrices = np.array(all_connectivity_matrices)\n",
    "    return all_connectivity_matrices\n",
    "\n",
    "def compute_mi_per_epoch_single(epoch_data, n_channels, n_neighbors=3):\n",
    "    \"\"\"\n",
    "    Helper function to compute Mutual Information for a single epoch.\n",
    "    Designed to be used with joblib.\n",
    "    \"\"\"\n",
    "    mi_matrix = np.zeros((n_channels, n_channels))\n",
    "    \n",
    "    # Iterate through unique pairs (upper triangle) to avoid redundant calculations\n",
    "    for ch1_idx in range(n_channels):\n",
    "        for ch2_idx in range(ch1_idx + 1, n_channels): # Start from ch1_idx + 1 for unique pairs\n",
    "            X = epoch_data[ch1_idx, :].reshape(-1, 1) # Reshape for sklearn\n",
    "            Y = epoch_data[ch2_idx, :]\n",
    "\n",
    "            # Using k-nearest neighbors method for MI estimation\n",
    "            mi = mutual_info_regression(X, Y, n_neighbors=n_neighbors, random_state=42)[0] # Add random_state for reproducibility\n",
    "            mi_matrix[ch1_idx, ch2_idx] = mi\n",
    "            mi_matrix[ch2_idx, ch1_idx] = mi # Ensure symmetry\n",
    "\n",
    "    return mi_matrix\n",
    "\n",
    "\n",
    "def compute_mutual_information_per_epoch(epochs, n_neighbors=3, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    Computes Mutual Information (MI) between all pairs of channels for each epoch,\n",
    "    using parallel processing for epochs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : mne.Epochs\n",
    "        The MNE Epochs object containing the data.\n",
    "    n_neighbors : int, optional\n",
    "        Number of nearest neighbors to use for MI estimation. The default is 3.\n",
    "    n_jobs : int, optional\n",
    "        Number of CPU cores to use for parallel processing across epochs.\n",
    "        -1 means use all available cores. The default is -1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_mi : numpy.ndarray\n",
    "        An array of MI matrices, with shape (n_epochs, n_channels, n_channels).\n",
    "        The diagonal will be 0 as MI of a signal with itself is not typically\n",
    "        calculated in this context or would be infinite for continuous data.\n",
    "    \"\"\"\n",
    "    n_epochs = len(epochs)\n",
    "    n_channels = len(epochs.ch_names)\n",
    "\n",
    "    print(f\"   Starting parallel MI computation for {n_epochs} epochs with {n_jobs} cores...\")\n",
    "\n",
    "    # Extract all epoch data into a single NumPy array for efficient processing\n",
    "    all_epochs_data = epochs.get_data(picks='eeg') # Shape: (n_epochs, n_channels, n_times)\n",
    "\n",
    "    # Use joblib to parallelize the computation across epochs\n",
    "    all_mi_matrices = Parallel(n_jobs=n_jobs)(\n",
    "        delayed(compute_mi_per_epoch_single)(all_epochs_data[i], n_channels, n_neighbors)\n",
    "        for i in range(n_epochs)\n",
    "    )\n",
    "\n",
    "    all_mi_matrices = np.array(all_mi_matrices)\n",
    "    \n",
    "    # Ensure diagonal is zero for all MI matrices\n",
    "    for mi_matrix in all_mi_matrices:\n",
    "        np.fill_diagonal(mi_matrix, 0.0)\n",
    "\n",
    "    return all_mi_matrices\n",
    "\n",
    "# --- NEW FUNCTION FOR PEARSON'S CORRELATION ---\n",
    "def compute_pearson_correlation_per_epoch(epochs):\n",
    "    \"\"\"\n",
    "    Computes Pearson's Correlation Coefficient for each epoch.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epochs : mne.Epochs\n",
    "        The MNE Epochs object containing the EEG data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        An array of Pearson correlation matrices, shape (n_epochs, n_channels, n_channels).\n",
    "        The diagonal will be 1.0 (correlation of a signal with itself).\n",
    "    \"\"\"\n",
    "    n_epochs = len(epochs)\n",
    "    n_channels = len(epochs.ch_names)\n",
    "    all_correlation_matrices = []\n",
    "\n",
    "    print(f\"   Computing Pearson's Correlation for {n_epochs} epochs...\")\n",
    "\n",
    "    for i, epoch_data in enumerate(epochs.get_data(picks='eeg')): # epoch_data shape: (channels, time_points)\n",
    "        correlation_matrix = np.eye(n_channels) # Initialize with identity for self-correlation = 1\n",
    "\n",
    "        for ch1_idx in range(n_channels):\n",
    "            for ch2_idx in range(ch1_idx + 1, n_channels): # Only compute upper triangle (symmetric matrix)\n",
    "                channel1_data = epoch_data[ch1_idx, :]\n",
    "                channel2_data = epoch_data[ch2_idx, :]\n",
    "\n",
    "                # Compute Pearson's r (we don't need the p-value here for the matrix)\n",
    "                correlation_coefficient, _ = pearsonr(channel1_data, channel2_data)\n",
    "\n",
    "                # Store in both (ch1, ch2) and (ch2, ch1) positions for symmetry\n",
    "                correlation_matrix[ch1_idx, ch2_idx] = correlation_coefficient\n",
    "                correlation_matrix[ch2_idx, ch1_idx] = correlation_coefficient\n",
    "        \n",
    "        all_correlation_matrices.append(correlation_matrix)\n",
    "    \n",
    "    return np.array(all_correlation_matrices)\n",
    "\n",
    "# --- Store connectivity results for each subject ---\n",
    "# This dictionary will now store COH, DPLI, MI, and Pearson's R for each subject and condition\n",
    "all_connectivity_by_subject = {}\n",
    "\n",
    "print(\"\\n--- Starting connectivity computation (COH, DPLI, MI, and Pearson's R) for all subjects ---\")\n",
    "for subject_id, epochs_data in epochs_by_subject.items():\n",
    "    print(f\"\\nComputing connectivity for {subject_id}...\")\n",
    "\n",
    "    epochs_att_current = epochs_data['epochs_att']\n",
    "    epochs_inatt_current = epochs_data['epochs_inatt']\n",
    "\n",
    "    # --- Compute COH ---\n",
    "    coh_att = compute_spectral_connectivity_per_epoch(epochs_att_current, method='coh')\n",
    "    print(f\"   {subject_id} Attention COH shape: {coh_att.shape}\")\n",
    "    coh_inatt = compute_spectral_connectivity_per_epoch(epochs_inatt_current, method='coh')\n",
    "    print(f\"   {subject_id} Inattention COH shape: {coh_inatt.shape}\")\n",
    "\n",
    "    # --- Compute DPLI ---\n",
    "    dpli_att = compute_spectral_connectivity_per_epoch(epochs_att_current, method='dpli')\n",
    "    print(f\"   {subject_id} Attention DPLI shape: {dpli_att.shape}\")\n",
    "    dpli_inatt = compute_spectral_connectivity_per_epoch(epochs_inatt_current, method='dpli')\n",
    "    print(f\"   {subject_id} Inattention DPLI shape: {dpli_inatt.shape}\")\n",
    "\n",
    "    # # --- Compute Mutual Information (MI) ---\n",
    "    mi_att = compute_mutual_information_per_epoch(epochs_att_current)\n",
    "    print(f\"   {subject_id} Attention MI shape: {mi_att.shape}\")\n",
    "    mi_inatt = compute_mutual_information_per_epoch(epochs_inatt_current)\n",
    "    print(f\"   {subject_id} Inattention MI shape: {mi_inatt.shape}\")\n",
    "\n",
    "    # # --- NEW: Compute Pearson's Correlation (R) ---\n",
    "    pearson_r_att = compute_pearson_correlation_per_epoch(epochs_att_current)\n",
    "    print(f\"   {subject_id} Attention Pearson's R shape: {pearson_r_att.shape}\")\n",
    "    pearson_r_inatt = compute_pearson_correlation_per_epoch(epochs_inatt_current)\n",
    "    print(f\"   {subject_id} Inattention Pearson's R shape: {pearson_r_inatt.shape}\")\n",
    "\n",
    "\n",
    "    # Store all results, including Pearson's R\n",
    "    all_connectivity_by_subject[subject_id] = {\n",
    "        'coh_att': coh_att,\n",
    "        'dpli_att': dpli_att,\n",
    "        'mi_att': mi_att,\n",
    "        'pearson_r_att': pearson_r_att, # Added Pearson's R here\n",
    "        'coh_inatt': coh_inatt,\n",
    "        'dpli_inatt': dpli_inatt,\n",
    "        'mi_inatt': mi_inatt,\n",
    "        'pearson_r_inatt': pearson_r_inatt # Added Pearson's R here\n",
    "    }\n",
    "\n",
    "print(\"\\n--- Finished connectivity computation for all subjects ---\")\n",
    "\n",
    "# --- Example of how to access the results (you can add visualization here) ---\n",
    "# For a specific subject and condition:\n",
    "# subject_example = 'S01'\n",
    "# if subject_example in all_connectivity_by_subject:\n",
    "#     # Access the array of Pearson's R matrices for Attention\n",
    "#     pearson_r_matrices_att_s01 = all_connectivity_by_subject[subject_example]['pearson_r_att']\n",
    "#     # You can then average them, or use them as per-epoch features\n",
    "#     avg_pearson_r_att_s01 = np.mean(pearson_r_matrices_att_s01, axis=0)\n",
    "#     print(f\"\\nAverage Pearson's R matrix for {subject_example} (Attention):\\n{avg_pearson_r_att_s01}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d375b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import networkx as nx # Assuming you have networkx imported for graph operations\n",
    "\n",
    "# --- Function to apply proportional thresholding to a connectivity matrix ---\n",
    "def proportional_threshold(matrix, density):\n",
    "    \"\"\"\n",
    "    Applies proportional thresholding to a connectivity matrix.\n",
    "    Assumes the input matrix is already a full, symmetric (or effectively symmetric for graph purposes)\n",
    "    matrix with its diagonal already set to the desired value (e.g., 0 or 1).\n",
    "    Retains edges corresponding to the top 'density' proportion of absolute edge weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy.ndarray\n",
    "        A 2D square connectivity matrix.\n",
    "    density : float\n",
    "        The desired density of the resulting graph (proportion of edges to keep).\n",
    "        Must be strictly between 0 and 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The thresholded matrix with values below the threshold set to 0.\n",
    "    \"\"\"\n",
    "    if not (0 < density < 1):\n",
    "        raise ValueError(\"Density must be strictly between 0 and 1.\")\n",
    "\n",
    "    # Use absolute values for thresholding, as weights are typically positive,\n",
    "    # or their magnitude is what matters for connectivity strength.\n",
    "    # We assume the matrix is already effectively symmetric and diagonal handled.\n",
    "    connectivity_values = np.abs(matrix)\n",
    "\n",
    "    # 1. Zero out the diagonal (if not already done or if you want to ensure it's 0 for thresholding)\n",
    "    # This is a good practice for graph connectivity to avoid self-loops influencing threshold.\n",
    "    np.fill_diagonal(connectivity_values, 0)\n",
    "\n",
    "    # 2. Extract unique off-diagonal elements from the matrix\n",
    "    # We only need the upper triangle because the matrix is already assumed symmetric for thresholding.\n",
    "    upper_tri_elements = connectivity_values[np.triu_indices_from(connectivity_values, k=1)]\n",
    "\n",
    "    if len(upper_tri_elements) == 0:\n",
    "        return np.zeros_like(matrix) # Handle case where matrix is all zeros or no unique edges\n",
    "\n",
    "    # 3. Determine the threshold value\n",
    "    # Sort weights in descending order and pick the value at the desired percentile\n",
    "    sorted_weights = np.sort(upper_tri_elements)[::-1] # Sort descending\n",
    "    \n",
    "    num_edges_to_keep = int(len(sorted_weights) * density)\n",
    "\n",
    "    if num_edges_to_keep == 0: # If density is too low to keep any edges\n",
    "        return np.zeros_like(matrix)\n",
    "\n",
    "    # The threshold is the value at the (num_edges_to_keep - 1)-th position in the sorted list (0-indexed)\n",
    "    threshold_value = sorted_weights[num_edges_to_keep - 1]\n",
    "\n",
    "    # 4. Apply thresholding: set values below threshold to 0\n",
    "    # Also, ensure values exactly equal to threshold are included\n",
    "    thresholded_mat = np.where(connectivity_values >= threshold_value, connectivity_values, 0)\n",
    "    \n",
    "    return thresholded_mat\n",
    "\n",
    "\n",
    "def threshold_and_convert_to_graph(connectivity_matrices, density=0.4):\n",
    "    \"\"\"\n",
    "    Applies proportional thresholding to a list of connectivity matrices\n",
    "    and converts each to a NetworkX graph, retaining weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    connectivity_matrices : list of np.ndarray or np.ndarray (if 3D array of matrices)\n",
    "        A list or array of connectivity matrices (e.g., from compute_connectivity_per_epoch).\n",
    "    density : float, optional\n",
    "        The desired density of the resulting graphs (proportion of edges to keep).\n",
    "        Must be between 0 and 1. Default is 0.4.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of networkx.Graph\n",
    "        A list of NetworkX graphs, one for each input matrix.\n",
    "    \"\"\"\n",
    "    graphs = []\n",
    "    # Ensure connectivity_matrices is iterable, even if it's a single 3D array\n",
    "    if isinstance(connectivity_matrices, np.ndarray) and connectivity_matrices.ndim == 3:\n",
    "        matrices_to_process = connectivity_matrices\n",
    "    else:\n",
    "        matrices_to_process = list(connectivity_matrices) # Ensure it's a list\n",
    "\n",
    "    for matrix in matrices_to_process:\n",
    "        # proportional_threshold handles abs and symmetry internally\n",
    "        thresholded = proportional_threshold(matrix, density)\n",
    "        \n",
    "        # Convert to graph, retaining weights. Ensure it's undirected.\n",
    "        # nx.from_numpy_array will create a weighted graph if the matrix contains non-binary values.\n",
    "        G = nx.from_numpy_array(thresholded)\n",
    "        \n",
    "        # Remove self-loops (proportional_threshold should already set diagonal to 0,\n",
    "        # but this is a good safeguard).\n",
    "        G.remove_edges_from(nx.selfloop_edges(G))\n",
    "        \n",
    "        graphs.append(G)\n",
    "    return graphs\n",
    "\n",
    "\n",
    "# Initialize a dictionary to store all types of graphs per subject\n",
    "# This will have a nested structure: graphs_by_subject[subject_id][connectivity_type_and_condition]\n",
    "graphs_by_subject = {}\n",
    "\n",
    "# --- Define a dictionary for preferred densities for each method ---\n",
    "# ADDED ENTRIES FOR PEARSON'S R\n",
    "method_densities = {\n",
    "    'coh_att': 0.5,\n",
    "    'dpli_att': 0.4,\n",
    "    'mi_att': 0.4,\n",
    "    'pearson_r_att': 0.4, # Added Pearson's R density for attention\n",
    "    'coh_inatt': 0.5,\n",
    "    'dpli_inatt': 0.4,\n",
    "    'mi_inatt': 0.4,\n",
    "    'pearson_r_inatt': 0.4, # Added Pearson's R density for inattention\n",
    "}\n",
    "\n",
    "# Iterate through each subject's data from the all_connectivity_by_subject dictionary\n",
    "print(\"\\n--- Starting graph conversion for all subjects and connectivity types ---\")\n",
    "for subject_id, data in all_connectivity_by_subject.items():\n",
    "    print(f\"\\nProcessing graphs for {subject_id}...\")\n",
    "    \n",
    "    # Initialize a sub-dictionary for the current subject's graphs\n",
    "    graphs_by_subject[subject_id] = {}\n",
    "\n",
    "    # Iterate through each connectivity type and its corresponding matrices for the current subject\n",
    "    # 'data.items()' will give you (key, value) pairs like ('coh_att', [coh_att_matrices])\n",
    "    for conn_type_key, connectivity_matrices_list in data.items():\n",
    "        # Get the appropriate density for the current connectivity type\n",
    "        # If the key is not in method_densities, it will use the default of 0.4\n",
    "        current_density = method_densities.get(conn_type_key, 0.4) \n",
    "\n",
    "        print(f\"   Processing {subject_id}: {conn_type_key} with density {current_density}...\")\n",
    "        \n",
    "        # Apply thresholding and convert to graphs\n",
    "        current_graphs = threshold_and_convert_to_graph(connectivity_matrices_list, density=current_density)\n",
    "        \n",
    "        # Store the list of graphs under the appropriate key for the subject\n",
    "        graphs_by_subject[subject_id][conn_type_key] = current_graphs\n",
    "        \n",
    "        print(f\"    {subject_id}: {len(current_graphs)} {conn_type_key} graphs created.\")\n",
    "\n",
    "print(\"\\n--- Finished graph conversion for all subjects ---\")\n",
    "\n",
    "# Example of how to access the processed graphs:\n",
    "# subject_id_example = list(graphs_by_subject.keys())[0] # Get the first subject ID\n",
    "# first_coh_att_graph = graphs_by_subject[subject_id_example]['coh_att'][0]\n",
    "# print(f\"\\nExample: First COH Attention graph for {subject_id_example}: {first_coh_att_graph}\")\n",
    "# print(f\"Number of nodes: {first_coh_att_graph.number_of_nodes()}, Number of edges: {first_coh_att_graph.number_of_edges()}\")\n",
    "\n",
    "# Accessing a Pearson's R graph:\n",
    "# if 'pearson_r_att' in graphs_by_subject[subject_id_example]:\n",
    "#     first_pearson_r_att_graph = graphs_by_subject[subject_id_example]['pearson_r_att'][0]\n",
    "#     print(f\"\\nExample: First Pearson's R Attention graph for {subject_id_example}: {first_pearson_r_att_graph}\")\n",
    "#     print(f\"Number of nodes: {first_pearson_r_att_graph.number_of_nodes()}, Number of edges: {first_pearson_r_att_graph.number_of_edges()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5cc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import community as community_louvain # Make sure this is installed (pip install python-louvain)\n",
    "from sklearn.impute import SimpleImputer # For handling NaNs\n",
    "\n",
    "# --- Updated extract_graph_features function ---\n",
    "def extract_graph_features(graphs):\n",
    "    \"\"\"\n",
    "    Extract traditional and advanced graph features from a list of NetworkX graphs.\n",
    "    Removed features that consistently returned NaN values based on previous analysis.\n",
    "    \"\"\"\n",
    "    all_features = []\n",
    "    feature_names = [\n",
    "        'num_nodes', 'num_edges', 'density',\n",
    "        'total_weight', 'avg_weight', 'min_weight', 'max_weight', 'std_weight',\n",
    "        'char_path_length', 'global_efficiency', 'diameter',\n",
    "        'avg_clustering_weighted', 'transitivity',\n",
    "        'avg_degree_centrality', 'avg_betweenness_centrality', 'avg_closeness_centrality',\n",
    "        'assortativity_degree', 'avg_weighted_degree',\n",
    "        'modularity', 'num_communities', 'max_community_size',\n",
    "        'spectral_radius', 'algebraic_connectivity',\n",
    "        'num_connected_components', 'size_largest_component_ratio',\n",
    "        'gini_coeff_degree', 'entropy_degree',\n",
    "        'avg_eccentricity', 'periphery_nodes_ratio', 'center_nodes_ratio',\n",
    "        'avg_k_core_size', 'max_k_core_size',\n",
    "        'node_connectivity', 'edge_connectivity',\n",
    "        'avg_shortest_path_unweighted',\n",
    "        'avg_degree_unweighted'\n",
    "    ] # Total 36 static features\n",
    "\n",
    "    for G in graphs:\n",
    "        feat = {name: np.nan for name in feature_names} # Initialize with NaN for all features\n",
    "\n",
    "        # Handle empty graphs or graphs with a single node\n",
    "        if not G or G.number_of_nodes() == 0:\n",
    "            feat['num_nodes'] = 0\n",
    "            feat['num_edges'] = 0\n",
    "            all_features.append(list(feat.values()))\n",
    "            continue\n",
    "        if G.number_of_nodes() == 1:\n",
    "            feat['num_nodes'] = 1\n",
    "            feat['num_edges'] = 0\n",
    "            all_features.append(list(feat.values()))\n",
    "            continue\n",
    "\n",
    "        feat['num_nodes'] = G.number_of_nodes()\n",
    "        feat['num_edges'] = G.number_of_edges()\n",
    "        feat['density'] = nx.density(G)\n",
    "\n",
    "        # Weighted features\n",
    "        if G.number_of_edges() > 0:\n",
    "            weights = [data['weight'] for u, v, data in G.edges(data=True)]\n",
    "            feat['total_weight'] = sum(weights)\n",
    "            feat['avg_weight'] = np.mean(weights)\n",
    "            feat['min_weight'] = np.min(weights)\n",
    "            feat['max_weight'] = np.max(weights)\n",
    "            feat['std_weight'] = np.std(weights)\n",
    "        else:\n",
    "            feat['total_weight'] = 0\n",
    "            feat['avg_weight'] = 0\n",
    "            feat['min_weight'] = np.nan\n",
    "            feat['max_weight'] = np.nan\n",
    "            feat['std_weight'] = np.nan\n",
    "\n",
    "        # Largest Connected Component (LCC) for path-based metrics\n",
    "        is_conn = nx.is_connected(G)\n",
    "        if not is_conn:\n",
    "            components = list(nx.connected_components(G))\n",
    "            G_lcc = G.subgraph(max(components, key=len))\n",
    "            feat['num_connected_components'] = len(components)\n",
    "            feat['size_largest_component_ratio'] = G_lcc.number_of_nodes() / G.number_of_nodes()\n",
    "        else:\n",
    "            G_lcc = G\n",
    "            feat['num_connected_components'] = 1\n",
    "            feat['size_largest_component_ratio'] = 1.0\n",
    "\n",
    "        # Path-based features (on LCC for robustness)\n",
    "        if G_lcc.number_of_nodes() > 1 and G_lcc.number_of_edges() > 0:\n",
    "            try:\n",
    "                feat['char_path_length'] = nx.average_shortest_path_length(G_lcc, weight='weight')\n",
    "                feat['avg_shortest_path_unweighted'] = nx.average_shortest_path_length(G_lcc)\n",
    "            except (nx.NetworkXError, nx.NetworkXPointlessConcept):\n",
    "                feat['char_path_length'] = np.nan\n",
    "                feat['avg_shortest_path_unweighted'] = np.nan\n",
    "\n",
    "            if not np.isnan(feat['char_path_length']) and feat['char_path_length'] != 0:\n",
    "                feat['global_efficiency'] = 1 / feat['char_path_length']\n",
    "            else:\n",
    "                feat['global_efficiency'] = np.nan\n",
    "\n",
    "            try:\n",
    "                feat['diameter'] = nx.diameter(G_lcc)\n",
    "                eccentricities = nx.eccentricity(G_lcc)\n",
    "                feat['avg_eccentricity'] = np.mean(list(eccentricities.values()))\n",
    "                periphery_nodes = nx.periphery(G_lcc)\n",
    "                center_nodes = nx.center(G_lcc)\n",
    "                feat['periphery_nodes_ratio'] = len(periphery_nodes) / G_lcc.number_of_nodes()\n",
    "                feat['center_nodes_ratio'] = len(center_nodes) / G_lcc.number_of_nodes()\n",
    "\n",
    "            except (nx.NetworkXError, nx.NetworkXPointlessConcept):\n",
    "                feat['diameter'] = np.nan\n",
    "                feat['avg_eccentricity'] = np.nan\n",
    "                feat['periphery_nodes_ratio'] = np.nan\n",
    "                feat['center_nodes_ratio'] = np.nan\n",
    "        else:\n",
    "            feat['char_path_length'] = np.nan\n",
    "            feat['global_efficiency'] = np.nan\n",
    "            feat['diameter'] = np.nan\n",
    "            feat['avg_eccentricity'] = np.nan\n",
    "            feat['periphery_nodes_ratio'] = np.nan\n",
    "            feat['center_nodes_ratio'] = np.nan\n",
    "            feat['avg_shortest_path_unweighted'] = np.nan\n",
    "\n",
    "        feat['avg_clustering_weighted'] = nx.average_clustering(G, weight='weight')\n",
    "        feat['transitivity'] = nx.transitivity(G)\n",
    "\n",
    "        # Removed 'avg_local_efficiency' as it consistently gave NaN\n",
    "\n",
    "        # Centrality measures (weighted averages)\n",
    "        if G.number_of_nodes() > 0:\n",
    "            degree_centrality = nx.degree_centrality(G)\n",
    "            feat['avg_degree_centrality'] = np.mean(list(degree_centrality.values()))\n",
    "\n",
    "            if G.number_of_nodes() > 2:\n",
    "                betweenness = nx.betweenness_centrality(G, weight='weight')\n",
    "                feat['avg_betweenness_centrality'] = np.mean(list(betweenness.values()))\n",
    "\n",
    "                closeness = nx.closeness_centrality(G, distance='weight')\n",
    "                feat['avg_closeness_centrality'] = np.mean(list(closeness.values()))\n",
    "            else:\n",
    "                feat['avg_betweenness_centrality'] = np.nan\n",
    "                feat['avg_closeness_centrality'] = np.nan\n",
    "        else:\n",
    "            feat.update({'avg_degree_centrality': np.nan, 'avg_betweenness_centrality': np.nan, 'avg_closeness_centrality': np.nan})\n",
    "\n",
    "        # Degree-related features (for weighted and unweighted degrees)\n",
    "        degrees = dict(G.degree(weight='weight'))\n",
    "        if G.number_of_nodes() > 0:\n",
    "            feat['avg_weighted_degree'] = sum(degrees.values()) / G.number_of_nodes()\n",
    "            feat['avg_degree_unweighted'] = np.mean(list(dict(G.degree()).values()))\n",
    "        else:\n",
    "            feat['avg_weighted_degree'] = 0\n",
    "            feat['avg_degree_unweighted'] = 0\n",
    "\n",
    "        # Assortativity\n",
    "        if G.number_of_edges() > 0:\n",
    "            try:\n",
    "                feat['assortativity_degree'] = nx.degree_assortativity_coefficient(G, weight='weight')\n",
    "            except Exception:\n",
    "                feat['assortativity_degree'] = np.nan\n",
    "        else:\n",
    "            feat['assortativity_degree'] = np.nan\n",
    "\n",
    "        # Community structure features (using Louvain method)\n",
    "        try:\n",
    "            if G.number_of_edges() > 0 and G.number_of_nodes() > 1:\n",
    "                partition = community_louvain.best_partition(G, weight='weight')\n",
    "                feat['modularity'] = community_louvain.modularity(partition, G, weight='weight')\n",
    "                num_communities = len(set(partition.values()))\n",
    "                feat['num_communities'] = num_communities\n",
    "                if num_communities > 0:\n",
    "                    community_sizes = [list(partition.values()).count(c) for c in set(partition.values())]\n",
    "                    feat['max_community_size'] = np.max(community_sizes)\n",
    "                else:\n",
    "                    feat['max_community_size'] = np.nan\n",
    "            else:\n",
    "                feat['modularity'] = np.nan\n",
    "                feat['num_communities'] = np.nan\n",
    "                feat['max_community_size'] = np.nan\n",
    "        except Exception as e:\n",
    "            feat['modularity'] = np.nan\n",
    "            feat['num_communities'] = np.nan\n",
    "            feat['max_community_size'] = np.nan\n",
    "\n",
    "        # Spectral features (eigenvalues of graph Laplacian)\n",
    "        if G.number_of_nodes() > 1:\n",
    "            try:\n",
    "                L = nx.normalized_laplacian_matrix(G, weight='weight')\n",
    "                eigenvalues = np.linalg.eigvalsh(L.toarray())\n",
    "                feat['spectral_radius'] = np.max(eigenvalues)\n",
    "                sorted_eigenvalues = np.sort(eigenvalues)\n",
    "                algebraic_connectivity = next((val for val in sorted_eigenvalues if val > 1e-9), 0)\n",
    "                feat['algebraic_connectivity'] = algebraic_connectivity\n",
    "            except Exception:\n",
    "                feat['spectral_radius'] = np.nan\n",
    "                feat['algebraic_connectivity'] = np.nan\n",
    "        else:\n",
    "            feat['spectral_radius'] = np.nan\n",
    "            feat['algebraic_connectivity'] = np.nan\n",
    "\n",
    "        # Distributional features (e.g., for degree distribution)\n",
    "        if G.number_of_nodes() > 1:\n",
    "            degrees_values = np.array(list(dict(G.degree(weight='weight')).values()))\n",
    "            if len(degrees_values) > 1:\n",
    "                sorted_degrees = np.sort(degrees_values)\n",
    "                n = len(sorted_degrees)\n",
    "                numerator = np.sum([(2 * (i + 1) - n - 1) * sorted_degrees[i] for i in range(n)])\n",
    "                denominator = n**2 * np.mean(sorted_degrees)\n",
    "                feat['gini_coeff_degree'] = numerator / denominator if denominator != 0 else np.nan\n",
    "\n",
    "                counts, bins = np.histogram(degrees_values, bins='auto', density=True)\n",
    "                counts = counts[counts > 0]\n",
    "                feat['entropy_degree'] = -np.sum(counts * np.log2(counts))\n",
    "            else:\n",
    "                feat['gini_coeff_degree'] = np.nan\n",
    "                feat['entropy_degree'] = np.nan\n",
    "        else:\n",
    "            feat['gini_coeff_degree'] = np.nan\n",
    "            feat['entropy_degree'] = np.nan\n",
    "\n",
    "        # Core-Periphery Measures\n",
    "        if G.number_of_nodes() > 1:\n",
    "            try:\n",
    "                k_core = nx.core_number(G)\n",
    "                if k_core:\n",
    "                    feat['avg_k_core_size'] = np.mean(list(k_core.values()))\n",
    "                    feat['max_k_core_size'] = np.max(list(k_core.values()))\n",
    "                else:\n",
    "                    feat['avg_k_core_size'] = np.nan\n",
    "                    feat['max_k_core_size'] = np.nan\n",
    "            except Exception:\n",
    "                feat['avg_k_core_size'] = np.nan\n",
    "                feat['max_k_core_size'] = np.nan\n",
    "        else:\n",
    "            feat['avg_k_core_size'] = np.nan\n",
    "            feat['max_k_core_size'] = np.nan\n",
    "\n",
    "        # Removed 'rich_club_coeff', 'small_world_omega', 'small_world_sigma' calculation blocks\n",
    "\n",
    "        # Robustness Measures\n",
    "        if G.number_of_nodes() > 1 and nx.is_connected(G):\n",
    "            try:\n",
    "                feat['node_connectivity'] = nx.node_connectivity(G)\n",
    "            except nx.NetworkXNoPath:\n",
    "                feat['node_connectivity'] = 0\n",
    "            except nx.NetworkXError:\n",
    "                feat['node_connectivity'] = np.nan\n",
    "            try:\n",
    "                feat['edge_connectivity'] = nx.edge_connectivity(G)\n",
    "            except nx.NetworkXNoPath:\n",
    "                feat['edge_connectivity'] = 0\n",
    "            except nx.NetworkXError:\n",
    "                feat['edge_connectivity'] = np.nan\n",
    "        else:\n",
    "            feat['node_connectivity'] = np.nan\n",
    "            feat['edge_connectivity'] = np.nan\n",
    "\n",
    "        # Ensure the order of features appended to all_features matches feature_names\n",
    "        ordered_features = [feat[name] for name in feature_names]\n",
    "        all_features.append(ordered_features)\n",
    "\n",
    "    return np.array(all_features), feature_names\n",
    "\n",
    "# --- Updated add_dynamic_features function ---\n",
    "def add_dynamic_features(graphs):\n",
    "    \"\"\"\n",
    "    Add temporal variability features from a list of NetworkX graphs.\n",
    "    Assumes 'graphs' is a list of graphs, ordered by time/epoch.\n",
    "    Each feature is calculated per epoch/graph, potentially relative to the previous.\n",
    "    \"\"\"\n",
    "    per_epoch_dynamic_features = [] # Initialize the list here!\n",
    "    dynamic_feature_names = [\n",
    "        'current_epoch_edge_weight_variability',\n",
    "        'current_epoch_mean_edge_weight',\n",
    "        'current_epoch_strong_connections_count',\n",
    "        'reconfiguration_jaccard_index',\n",
    "        'frobenius_norm_diff',\n",
    "        'delta_mean_degree',\n",
    "        'delta_mean_clustering'\n",
    "    ] # Total 7 dynamic features\n",
    "\n",
    "    if not graphs:\n",
    "        # If no graphs are provided, return NaNs for all dynamic features\n",
    "        return np.full((1, len(dynamic_feature_names)), np.nan), dynamic_feature_names\n",
    "\n",
    "    for i, G in enumerate(graphs):\n",
    "        # Features from your original add_dynamic_features, now explicitly per epoch\n",
    "        weights = [G[u][v]['weight'] for u, v in G.edges()]\n",
    "        if len(weights) > 0:\n",
    "            ew_variability = np.std(weights) if len(weights) > 1 else 0\n",
    "            mean_ew = np.mean(weights)\n",
    "            strong_conn_count = sum(1 for w in weights if w > 0.5) # Example threshold\n",
    "        else:\n",
    "            ew_variability = np.nan\n",
    "            mean_ew = np.nan\n",
    "            strong_conn_count = np.nan\n",
    "\n",
    "        # New dynamic features based on temporal evolution (comparison to previous epoch)\n",
    "        jaccard_index = np.nan\n",
    "        frobenius_diff = np.nan\n",
    "        delta_mean_degree = np.nan\n",
    "        delta_mean_clustering = np.nan\n",
    "\n",
    "        if i > 0: # Only calculate these for 2nd epoch onwards\n",
    "            G_prev = graphs[i-1]\n",
    "\n",
    "            # Reconfiguration Index: Jaccard similarity of edge sets (unweighted)\n",
    "            edges_current = set(G.edges())\n",
    "            edges_prev = set(G_prev.edges())\n",
    "            union_edges = len(edges_current.union(edges_prev))\n",
    "            if union_edges > 0:\n",
    "                intersection_edges = len(edges_current.intersection(edges_prev))\n",
    "                jaccard_index = 1 - (intersection_edges / union_edges) # 1 - similarity = dissimilarity\n",
    "            else: # Both empty or one empty\n",
    "                jaccard_index = 0 if not edges_current and not edges_prev else np.nan\n",
    "\n",
    "            # Frobenius Norm of Adjacency Matrix Difference\n",
    "            # Assumes consistent node ordering across epochs\n",
    "            nodes = sorted(list(G.nodes())) # Ensure consistent node order for matrix conversion\n",
    "            if not nodes: # Handle case of empty graphs\n",
    "                frobenius_diff = np.nan\n",
    "            else:\n",
    "                adj_current = nx.to_numpy_array(G, nodelist=nodes, weight='weight')\n",
    "                adj_prev = nx.to_numpy_array(G_prev, nodelist=nodes, weight='weight')\n",
    "\n",
    "                if adj_current.shape == adj_prev.shape:\n",
    "                    frobenius_diff = np.linalg.norm(adj_current - adj_prev, 'fro')\n",
    "                else:\n",
    "                    frobenius_diff = np.nan # Should ideally not happen with fixed channels\n",
    "\n",
    "            # Changes in global metrics (simple difference from previous epoch)\n",
    "            current_avg_degree = np.mean(list(dict(G.degree(weight='weight')).values())) if G.number_of_nodes() > 0 else np.nan\n",
    "            prev_avg_degree = np.mean(list(dict(G_prev.degree(weight='weight')).values())) if G_prev.number_of_nodes() > 0 else np.nan\n",
    "            delta_mean_degree = current_avg_degree - prev_avg_degree if not np.isnan(current_avg_degree) and not np.isnan(prev_avg_degree) else np.nan\n",
    "\n",
    "            current_avg_clustering = nx.average_clustering(G, weight='weight') if G.number_of_nodes() > 0 else np.nan\n",
    "            prev_avg_clustering = nx.average_clustering(G_prev, weight='weight') if G_prev.number_of_nodes() > 0 else np.nan\n",
    "            delta_mean_clustering = current_avg_clustering - prev_avg_clustering if not np.isnan(current_avg_clustering) and not np.isnan(prev_avg_clustering) else np.nan\n",
    "\n",
    "        per_epoch_dynamic_features.append([\n",
    "            ew_variability,\n",
    "            mean_ew,\n",
    "            strong_conn_count,\n",
    "            jaccard_index,\n",
    "            frobenius_diff,\n",
    "            delta_mean_degree,\n",
    "            delta_mean_clustering\n",
    "        ])\n",
    "\n",
    "    return np.array(per_epoch_dynamic_features), dynamic_feature_names\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a0eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ttest_ind\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "# Iterate over each connectivity type (COH, DPLI, and MI)\n",
    "# --- The only change needed is adding 'mi' to this list ---\n",
    "for conn_type in ['coh', 'mi','dpli','pearson_r']: # ADDED 'mi' here!\n",
    "    print(f\"\\n--- Starting analysis for {conn_type.upper()} connectivity ---\")\n",
    "\n",
    "    # Initialize dictionaries for the current connectivity type\n",
    "    features_by_subject = {}\n",
    "    labels_by_subject = {}\n",
    "    combined_features_by_subject = {}\n",
    "    combined_labels_by_subject = {}\n",
    "    all_feature_names = [] # This will be populated once for each conn_type\n",
    "\n",
    "\n",
    "\n",
    "    # Before the loop, check if graphs_by_subject exists and is not empty.\n",
    "    # If it's empty or not defined, the loop over its keys won't run.\n",
    "    if not graphs_by_subject:\n",
    "        print(\"Error: 'graphs_by_subject' is empty or not defined. Please ensure graph conversion ran successfully.\")\n",
    "        continue # Skip this conn_type if no data to process\n",
    "\n",
    "    for subject_id in graphs_by_subject.keys():\n",
    "        # Access graphs specific to the current connectivity type and condition\n",
    "        # This line automatically picks up 'mi_att' and 'mi_inatt' when conn_type is 'mi'\n",
    "        graphs_att = graphs_by_subject[subject_id].get(f'{conn_type}_att')\n",
    "        graphs_inatt = graphs_by_subject[subject_id].get(f'{conn_type}_inatt')\n",
    "\n",
    "        # Handle cases where a specific connectivity type might not exist for a subject\n",
    "        if graphs_att is None or graphs_inatt is None:\n",
    "            print(f\"Warning: {conn_type.upper()} data not found for subject {subject_id}. Skipping this subject for {conn_type}.\")\n",
    "            continue\n",
    "\n",
    "        # Extract static features\n",
    "        X_att_static, static_names = extract_graph_features(graphs_att)\n",
    "        X_inatt_static, _ = extract_graph_features(graphs_inatt)\n",
    "\n",
    "        # Extract dynamic features\n",
    "        X_att_dyn, dynamic_names = add_dynamic_features(graphs_att)\n",
    "        X_inatt_dyn, _ = add_dynamic_features(graphs_inatt)\n",
    "\n",
    "        # Populate feature names only once (assuming feature names are consistent across subjects)\n",
    "        if not all_feature_names:\n",
    "            # Add prefix to feature names here for clarity\n",
    "            prefixed_static_names = [f\"{conn_type}_{name}\" for name in static_names]\n",
    "            prefixed_dynamic_names = [f\"{conn_type}_{name}\" for name in dynamic_names]\n",
    "            all_feature_names = prefixed_static_names + prefixed_dynamic_names\n",
    "\n",
    "        # Combine features\n",
    "        # Robustly combine static and dynamic features, handling cases where one might be empty\n",
    "        # A subject could have no features if, for instance, `extract_graph_features` returns empty for sparse graphs.\n",
    "        current_X_att = []\n",
    "        current_X_inatt = []\n",
    "\n",
    "        if X_att_static.shape[1] > 0:\n",
    "            current_X_att.append(X_att_static)\n",
    "            current_X_inatt.append(X_inatt_static)\n",
    "        if X_att_dyn.shape[1] > 0:\n",
    "            current_X_att.append(X_att_dyn)\n",
    "            current_X_inatt.append(X_inatt_dyn)\n",
    "\n",
    "        if not current_X_att or not current_X_inatt:\n",
    "            print(f\"Warning: No features (static or dynamic) extracted for {subject_id} for {conn_type}. Skipping subject.\")\n",
    "            continue # Skip to next subject if no features are extracted\n",
    "\n",
    "        X_att = np.hstack(current_X_att)\n",
    "        X_inatt = np.hstack(current_X_inatt)\n",
    "\n",
    "        # Impute missing values\n",
    "        # Fit imputer on combined data for consistent scaling and imputation\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_combined_for_imputation = np.vstack((X_att, X_inatt))\n",
    "\n",
    "        # Check if there are any features to impute\n",
    "        if X_combined_for_imputation.shape[1] == 0:\n",
    "            print(f\"Warning: No features to impute for {subject_id} for {conn_type}. Skipping imputation for this subject.\")\n",
    "            # Assign empty arrays if no features\n",
    "            X_att_imputed = X_att\n",
    "            X_inatt_imputed = X_inatt\n",
    "        else:\n",
    "            X_combined_imputed = imputer.fit_transform(X_combined_for_imputation)\n",
    "            n_att = X_att.shape[0] # Use original shape for splitting\n",
    "            X_att_imputed = X_combined_imputed[:n_att]\n",
    "            X_inatt_imputed = X_combined_imputed[n_att:]\n",
    "\n",
    "\n",
    "        # Z-score normalization across both conditions\n",
    "        scaler = StandardScaler()\n",
    "        # Check if there are any features to scale\n",
    "        if X_att_imputed.shape[1] == 0:\n",
    "            print(f\"Warning: No features to scale for {subject_id} for {conn_type}. Skipping scaling for this subject.\")\n",
    "            X_att_scaled = X_att_imputed\n",
    "            X_inatt_scaled = X_inatt_imputed\n",
    "        else:\n",
    "            X_combined_scaled = scaler.fit_transform(np.vstack((X_att_imputed, X_inatt_imputed)))\n",
    "            n_att = X_att_imputed.shape[0] # Use imputed shape for splitting\n",
    "            X_att_scaled = X_combined_scaled[:n_att]\n",
    "            X_inatt_scaled = X_combined_scaled[n_att:]\n",
    "\n",
    "\n",
    "        features_by_subject[subject_id] = {\n",
    "            'attention': X_att_scaled,\n",
    "            'inattention': X_inatt_scaled\n",
    "        }\n",
    "\n",
    "        labels_att = np.ones(X_att_scaled.shape[0])\n",
    "        labels_inatt = np.zeros(X_inatt_scaled.shape[0])\n",
    "\n",
    "        labels_by_subject[subject_id] = {\n",
    "            'attention': labels_att,\n",
    "            'inattention': labels_inatt\n",
    "        }\n",
    "\n",
    "        # Combine for t-test usage\n",
    "        combined_feats = np.vstack((X_att_scaled, X_inatt_scaled))\n",
    "        combined_labels = np.concatenate((labels_att, labels_inatt))\n",
    "\n",
    "        combined_features_by_subject[subject_id] = combined_feats\n",
    "        combined_labels_by_subject[subject_id] = combined_labels\n",
    "\n",
    "    print(f\"Extracted features for {len(features_by_subject)} subjects for {conn_type.upper()}.\")\n",
    "    print(f\"Total number of features extracted for {conn_type.upper()}: {len(all_feature_names)}\")\n",
    "\n",
    "    # === Combine all subjects' data for t-test ===\n",
    "    # Check if any features were extracted at all for this connectivity type\n",
    "    if not features_by_subject:\n",
    "        print(f\"No features available for {conn_type.upper()}. Skipping statistical analysis for this type.\")\n",
    "        analysis_results[conn_type] = {\n",
    "            't_values': np.array([]), 'p_values': np.array([]),\n",
    "            'corrected_pvals': np.array([]), 'significant_mask': np.array([]),\n",
    "            'cohens_d_values': []\n",
    "        } # Store empty arrays to avoid key errors later\n",
    "        continue # Skip to the next connectivity type\n",
    "\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "\n",
    "    for subject_id in combined_features_by_subject:\n",
    "        if combined_features_by_subject[subject_id].shape[0] > 0: # Only add if data exists for subject\n",
    "            all_feats.append(combined_features_by_subject[subject_id])\n",
    "            all_labels.append(combined_labels_by_subject[subject_id])\n",
    "    \n",
    "    if not all_feats:\n",
    "        print(f\"No combined features from any subject for {conn_type.upper()}. Skipping statistical analysis.\")\n",
    "        analysis_results[conn_type] = {\n",
    "            't_values': np.array([]), 'p_values': np.array([]),\n",
    "            'corrected_pvals': np.array([]), 'significant_mask': np.array([]),\n",
    "            'cohens_d_values': []\n",
    "        }\n",
    "        continue\n",
    "\n",
    "    X_all = np.vstack(all_feats)\n",
    "    y_all = np.concatenate(all_labels)\n",
    "\n",
    "    print(f\"Combined data shape for {conn_type.upper()}: {X_all.shape}, Labels shape: {y_all.shape}\")\n",
    "\n",
    "    # Separate attention and inattention data\n",
    "    X_att = X_all[y_all == 1]\n",
    "    X_inatt = X_all[y_all == 0]\n",
    "\n",
    "    # Handle any residual NaNs (safety)\n",
    "    X_att = np.nan_to_num(X_att, nan=0.0)\n",
    "    X_inatt = np.nan_to_num(X_inatt, nan=0.0)\n",
    "\n",
    "    # === Perform t-test ===\n",
    "    # Check if there are enough samples and features for t-test\n",
    "    if X_att.shape[0] < 2 or X_inatt.shape[0] < 2 or X_all.shape[1] == 0:\n",
    "        print(f\"Not enough samples ({X_att.shape[0]} att, {X_inatt.shape[0]} inatt) or no features ({X_all.shape[1]}) for t-test for {conn_type.upper()}. Skipping.\")\n",
    "        t_values = np.array([])\n",
    "        p_values = np.array([])\n",
    "        significant_mask = np.array([])\n",
    "        corrected_pvals = np.array([])\n",
    "    else:\n",
    "        t_values, p_values = ttest_ind(X_att, X_inatt, axis=0, equal_var=False)\n",
    "\n",
    "        # Bonferroni correction\n",
    "        if len(p_values) == 0: # Check if p_values is empty (e.g., no features)\n",
    "            significant_mask = np.array([])\n",
    "            corrected_pvals = np.array([])\n",
    "        else:\n",
    "            significant_mask, corrected_pvals, _, _ = multipletests(p_values, alpha=0.05, method='bonferroni')\n",
    "    \n",
    "    n_significant = np.sum(significant_mask)\n",
    "    print(f\"\\nNumber of significant features for {conn_type.upper()}: {n_significant} / {len(p_values)}\")\n",
    "\n",
    "    # === Show top significant features ===\n",
    "    top_k = 10\n",
    "    top_indices = []\n",
    "    if len(p_values) > 0: # Only proceed if there are p-values\n",
    "        # Sort by p-value\n",
    "        sorted_p_indices = np.argsort(p_values)\n",
    "        \n",
    "        # Filter for significant features and take top_k\n",
    "        significant_indices_sorted = [idx for idx in sorted_p_indices if significant_mask[idx]]\n",
    "        \n",
    "        if len(significant_indices_sorted) > 0:\n",
    "            top_indices = significant_indices_sorted[:top_k]\n",
    "        elif len(all_feature_names) > 0: # If no significant features, take top_k overall (if features exist)\n",
    "            top_indices = sorted_p_indices[:top_k]\n",
    "\n",
    "    if top_indices:\n",
    "        print(f\"\\nTop {top_k} features for {conn_type.upper()} (by p-value):\")\n",
    "        for idx in top_indices:\n",
    "            print(f\"   {idx:3d} | p={p_values[idx]:.4e} | Corrected p={corrected_pvals[idx]:.4e} | Significant: {significant_mask[idx]} | {all_feature_names[idx]}\")\n",
    "    else:\n",
    "        print(f\"No features to display for {conn_type.upper()}.\")\n",
    "\n",
    "    # === Plot p-values and significant features ===\n",
    "    if len(p_values) > 0: # Only plot if there are p-values\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(p_values, marker='o', linestyle='-', alpha=0.5, label='p-values')\n",
    "        if n_significant > 0:\n",
    "            plt.plot(np.where(significant_mask)[0], p_values[significant_mask], 'ro', label='Significant (Bonferroni)')\n",
    "        plt.axhline(y=0.05, color='red', linestyle='--', label='p=0.05 (uncorrected)')\n",
    "        if len(p_values) > 0:\n",
    "            plt.axhline(y=0.05 / len(p_values), color='green', linestyle=':', label='Bonferroni threshold')\n",
    "        plt.title(f\"p-values for all features ({conn_type.upper()}: Attention vs Inattention)\")\n",
    "        plt.xlabel(\"Feature Index\")\n",
    "        plt.ylabel(\"p-value\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No p-values to plot for {conn_type.upper()}.\")\n",
    "\n",
    "    # === Compute Cohen's d ===\n",
    "    def compute_cohens_d(x1, x2):\n",
    "        n1, n2 = len(x1), len(x2)\n",
    "        # Handle cases where variance might be zero or very small to avoid division by zero\n",
    "        var1 = np.var(x1, ddof=1)\n",
    "        var2 = np.var(x2, ddof=1)\n",
    "        \n",
    "        pooled_std = np.sqrt(((n1-1)*var1 + (n2-1)*var2) / (n1 + n2 - 2 + 1e-9))\n",
    "        \n",
    "        if pooled_std == 0:\n",
    "            return 0.0\n",
    "        return (np.mean(x1) - np.mean(x2)) / pooled_std\n",
    "\n",
    "    cohens_d_values = []\n",
    "    if X_all.shape[1] > 0 and X_att.shape[0] > 0 and X_inatt.shape[0] > 0: # Only compute if there are features and data\n",
    "        cohens_d_values = [compute_cohens_d(X_att[:, i], X_inatt[:, i]) for i in range(X_all.shape[1])]\n",
    "\n",
    "    # === Store results for the current connectivity type ===\n",
    "    analysis_results[conn_type] = {\n",
    "        'features_by_subject': features_by_subject,\n",
    "        'labels_by_subject': labels_by_subject,\n",
    "        'combined_features_by_subject': combined_features_by_subject,\n",
    "        'combined_labels_by_subject': combined_labels_by_subject,\n",
    "        'all_feature_names': all_feature_names,\n",
    "        't_values': t_values,\n",
    "        'p_values': p_values,\n",
    "        'corrected_pvals': corrected_pvals,\n",
    "        'significant_mask': significant_mask,\n",
    "        'cohens_d_values': cohens_d_values\n",
    "    }\n",
    "\n",
    "    # === Example for one feature (boxplot) ===\n",
    "    # Only plot if there are features, feature names, and enough data points for both conditions\n",
    "    if len(all_feature_names) > 0 and X_att.shape[0] > 0 and X_inatt.shape[0] > 0:\n",
    "        example_feat_idx = -1\n",
    "        significant_indices = np.where(analysis_results[conn_type]['significant_mask'])[0]\n",
    "        \n",
    "        if len(significant_indices) > 0:\n",
    "            example_feat_idx = significant_indices[0] # Pick the first significant feature\n",
    "        elif len(all_feature_names) > 0:\n",
    "            example_feat_idx = 0 # If no significant, pick the first feature\n",
    "\n",
    "        if example_feat_idx != -1 and example_feat_idx < X_att.shape[1]: # Ensure index is valid\n",
    "            df = pd.DataFrame({\n",
    "                'Feature Value': np.concatenate([X_att[:, example_feat_idx], X_inatt[:, example_feat_idx]]),\n",
    "                'Label': ['Attention'] * len(X_att) + ['Inattention'] * len(X_inatt)\n",
    "            })\n",
    "\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.boxplot(data=df, x='Label', y='Feature Value')\n",
    "            plt.title(f'Distribution of {all_feature_names[example_feat_idx]} for {conn_type.upper()}')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Not enough data or valid feature index to plot example boxplot for {conn_type.upper()}.\")\n",
    "    else:\n",
    "        print(f\"No features or data to plot example boxplot for {conn_type.upper()}.\")\n",
    "\n",
    "print(\"\\n--- Finished analysis for all connectivity types ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410e90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier # Needed for AdaBoost base_estimator\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def ttest_feature_selection(X_train, y_train, X_test, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs feature selection using independent t-tests.\n",
    "    Selects features where the p-value is below the given alpha threshold.\n",
    "    Includes a fallback to select top 10 features if none pass the threshold.\n",
    "    \"\"\"\n",
    "    X_att = X_train[y_train == 1]\n",
    "    X_inatt = X_train[y_train == 0]\n",
    "\n",
    "    # Handle cases where one class might be empty or not enough samples for t-test\n",
    "    if X_att.shape[0] < 2 or X_inatt.shape[0] < 2:\n",
    "        print(\"Warning: Not enough samples in one or both classes for t-test. Skipping t-test based selection, returning all features.\")\n",
    "        if X_train.shape[1] > 0:\n",
    "            return X_train, X_test, np.arange(X_train.shape[1])\n",
    "        else: # No features to begin with\n",
    "            return np.array([]).reshape(X_train.shape[0], 0), \\\n",
    "                   np.array([]).reshape(X_test.shape[0], 0), \\\n",
    "                   np.array([])\n",
    "\n",
    "    if X_train.shape[1] == 0:\n",
    "        print(\"Warning: No features in input to t-test selection. Returning empty arrays.\")\n",
    "        return np.array([]).reshape(X_train.shape[0], 0), \\\n",
    "               np.array([]).reshape(X_test.shape[0], 0), \\\n",
    "               np.array([])\n",
    "\n",
    "    t_vals, p_vals = ttest_ind(X_att, X_inatt, axis=0, equal_var=False)\n",
    "    selected_indices = np.where(p_vals < alpha)[0]\n",
    "\n",
    "    # Fallback if no features pass threshold or if t-test was skipped\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Warning: No features passed t-test threshold (alpha={alpha}). Selecting top 10 features by p-value.\")\n",
    "        if p_vals.size > 0: # Ensure p_vals is not empty before sorting\n",
    "            selected_indices = np.argsort(p_vals)[:min(10, p_vals.size)] # Select top 10 or all if less than 10\n",
    "        else: # If no p-values (e.g., no features to begin with)\n",
    "            selected_indices = np.array([])\n",
    "        \n",
    "        # Final fallback to select all features if nothing else worked and features exist\n",
    "        if len(selected_indices) == 0 and X_train.shape[1] > 0:\n",
    "            selected_indices = np.arange(X_train.shape[1])\n",
    "\n",
    "    if X_train.shape[1] == 0 or len(selected_indices) == 0:\n",
    "        print(\"Warning: No features left after t-test selection. Returning empty arrays.\")\n",
    "        return np.array([]).reshape(X_train.shape[0], 0), \\\n",
    "               np.array([]).reshape(X_test.shape[0], 0), \\\n",
    "               np.array([])\n",
    "\n",
    "    return X_train[:, selected_indices], X_test[:, selected_indices], selected_indices\n",
    "\n",
    "# Z-score normalize features per subject (not used in main loop, StandardScaler is)\n",
    "def zscore_per_subject(X):\n",
    "    if X.shape[0] == 0 or X.shape[1] == 0:\n",
    "        return X\n",
    "    \n",
    "    mean = X.mean(axis=0, keepdims=True)\n",
    "    std = X.std(axis=0, keepdims=True)\n",
    "    std[std == 0] = 1 # Avoid division by zero\n",
    "    return (X - mean) / std\n",
    "\n",
    "# --- Models and hyperparameter grids (Optimized for speed) ---\n",
    "models_and_params = {\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=2000, random_state=42), {\n",
    "        'C': [0.001, 0.01, 0.1, 1], # Reduced range\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        # 'saga' supports all penalties including elasticnet. 'liblinear' supports l1/l2.\n",
    "        # 'lbfgs' only supports l2 and no l1_ratio.\n",
    "        'solver': ['saga'], # Focus on saga for elasticnet\n",
    "        'l1_ratio': [0.1, 0.5, 0.9], # For elasticnet\n",
    "        'max_iter': [1000] # Single value, good enough if convergence is met\n",
    "    }),\n",
    "    \"SGD Classifier\": (SGDClassifier(random_state=42), {\n",
    "        'loss': ['hinge', 'log_loss', 'modified_huber'], # 'log' is deprecated, use 'log_loss'\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'alpha': [0.0001, 0.001, 0.01], # Reduced range\n",
    "        'l1_ratio': [0.3, 0.7],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'eta0': [0.01, 0.1]\n",
    "    }),\n",
    "    'MLP': (MLPClassifier(max_iter=2000, random_state=42), {\n",
    "        'hidden_layer_sizes': [\n",
    "            (100,), (150,), # Single layer\n",
    "            (100, 50) # Two layers\n",
    "        ],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam'], # Adam is generally faster\n",
    "        'alpha': [0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'learning_rate_init': [0.0001, 0.001],\n",
    "    }),\n",
    "    'RandomForest': (RandomForestClassifier(random_state=42), {\n",
    "        'n_estimators': [100, 200], # Fewer options\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4],\n",
    "        'max_features': ['sqrt', None], # Simplified options\n",
    "        'bootstrap': [True],\n",
    "        'criterion': ['gini']\n",
    "    }),\n",
    "    \"Extra Trees\": (ExtraTreesClassifier(random_state=42), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 20],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4],\n",
    "        'max_features': ['sqrt', None],\n",
    "        'criterion': ['gini']\n",
    "    }),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(random_state=42), {\n",
    "        'n_estimators': [100], # Fewer options\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1],\n",
    "        'max_features': ['sqrt']\n",
    "    }),\n",
    "    \"XGBoost\": (XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, tree_method='hist'), { # Removed device='cuda' for broader compatibility\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 0.2],\n",
    "        'reg_alpha': [0, 0.1],\n",
    "        'reg_lambda': [0.1, 1]\n",
    "    }),\n",
    "  \n",
    "    'SVM': (SVC(probability=True, random_state=42), {\n",
    "        'C': [0.1, 1], # Reduced range\n",
    "        'kernel': ['rbf'], # Focus on most common kernel\n",
    "        'gamma': ['scale', 0.1],\n",
    "        'shrinking': [True],\n",
    "        'class_weight': [None] # Reduced option\n",
    "    }),\n",
    "    \"Gaussian Naive Bayes\": (GaussianNB(), {\n",
    "        'var_smoothing': [1e-9, 1e-7] # Reduced options\n",
    "    }),\n",
    "  \n",
    "    \"Bernoulli Naive Bayes\": (BernoulliNB(), {\n",
    "        'alpha': [0.1, 1.0],\n",
    "        'binarize': [0.0], # A typical binarization threshold\n",
    "        'fit_prior': [True, False]\n",
    "    })\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\n--- Combining COH, DPLI, pearson_r and MI features and names for each subject ---\")\n",
    "combined_all_features_by_subject = {}\n",
    "combined_all_labels_by_subject = {}\n",
    "\n",
    "all_subject_ids_sets = []\n",
    "for modality in ['coh', 'dpli', 'mi', 'pearson_r']:\n",
    "    if modality in analysis_results and 'combined_features_by_subject' in analysis_results[modality]:\n",
    "        all_subject_ids_sets.append(set(analysis_results[modality]['combined_features_by_subject'].keys()))\n",
    "\n",
    "if not all_subject_ids_sets:\n",
    "    print(\"Error: No connectivity results found in 'analysis_results'. Please ensure previous steps ran successfully.\")\n",
    "    # If analysis_results is truly empty, the rest of the script will likely fail gracefully with warnings/empty results.\n",
    "    # For a runnable script, we'll proceed assuming dummy data is enough.\n",
    "\n",
    "subject_ids = sorted(list(set.union(*all_subject_ids_sets)))\n",
    "\n",
    "# Get feature names for COH, DPLI, MI, and pearson_r\n",
    "coh_feature_names = analysis_results.get('coh', {}).get('all_feature_names', [])\n",
    "dpli_feature_names = analysis_results.get('dpli', {}).get('all_feature_names', [])\n",
    "mi_feature_names = analysis_results.get('mi', {}).get('all_feature_names', [])\n",
    "pearson_r_feature_names = analysis_results.get('pearson_r', {}).get('all_feature_names', [])\n",
    "\n",
    "# Create the combined feature names list\n",
    "combined_all_feature_names = coh_feature_names + dpli_feature_names + mi_feature_names + pearson_r_feature_names\n",
    "\n",
    "# Determine max_epochs for padding zero arrays if any modality is missing for a subject.\n",
    "max_epochs = 0\n",
    "for sub_id in subject_ids:\n",
    "    for modality in ['coh', 'dpli', 'mi', 'pearson_r']:\n",
    "        if modality in analysis_results and sub_id in analysis_results[modality].get('combined_features_by_subject', {}):\n",
    "            num_epochs = analysis_results[modality]['combined_features_by_subject'][sub_id].shape[0]\n",
    "            if num_epochs > max_epochs:\n",
    "                max_epochs = num_epochs\n",
    "\n",
    "if max_epochs == 0:\n",
    "    print(\"Warning: No epochs found across any subject or modality. Classification might fail.\")\n",
    "    # If no epochs, padding with zeros might be problematic or lead to empty arrays later.\n",
    "    # For robust demonstration, let's ensure max_epochs is at least 1 if there are any features.\n",
    "    if any(len(feat_names_list) > 0 for feat_names_list in [coh_feature_names, dpli_feature_names, mi_feature_names, pearson_r_feature_names]):\n",
    "        print(\"Forcing max_epochs to 1 as no epochs found but feature names exist.\")\n",
    "        max_epochs = 1 # Minimal epoch for the sake of array creation if truly no data.\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    print(f\"Processing subject {subject_id} for combined features...\")\n",
    "    subject_features_to_stack = []\n",
    "    subject_labels = None # Labels should ideally be consistent across modalities for a subject\n",
    "\n",
    "    for modality, feat_names_list in zip(['coh', 'dpli', 'mi', 'pearson_r'],\n",
    "                                         [coh_feature_names, dpli_feature_names, mi_feature_names, pearson_r_feature_names]):\n",
    "        \n",
    "        features_for_modality = analysis_results.get(modality, {}).get('combined_features_by_subject', {}).get(subject_id)\n",
    "        labels_for_modality = analysis_results.get(modality, {}).get('combined_labels_by_subject', {}).get(subject_id)\n",
    "\n",
    "        if features_for_modality is not None and features_for_modality.shape[0] > 0:\n",
    "            # Ensure features have correct number of columns\n",
    "            if features_for_modality.shape[1] != len(feat_names_list):\n",
    "                print(f\"  Warning: Feature count mismatch for {modality} of {subject_id}. Expected {len(feat_names_list)}, got {features_for_modality.shape[1]}. Skipping or padding might occur.\")\n",
    "                # Attempt to pad/truncate if mismatch, or raise error\n",
    "                # For robustness, we'll try to align to the expected number of features\n",
    "                if features_for_modality.shape[1] > len(feat_names_list):\n",
    "                    features_for_modality = features_for_modality[:, :len(feat_names_list)]\n",
    "                else: # Pad with zeros if fewer features than expected\n",
    "                    temp_features = np.zeros((features_for_modality.shape[0], len(feat_names_list)))\n",
    "                    temp_features[:, :features_for_modality.shape[1]] = features_for_modality\n",
    "                    features_for_modality = temp_features\n",
    "\n",
    "            # Pad or truncate features to max_epochs for consistent stacking\n",
    "            if features_for_modality.shape[0] < max_epochs:\n",
    "                padded_features = np.zeros((max_epochs, features_for_modality.shape[1]))\n",
    "                padded_features[:features_for_modality.shape[0], :] = features_for_modality\n",
    "                subject_features_to_stack.append(padded_features)\n",
    "            elif features_for_modality.shape[0] > max_epochs:\n",
    "                subject_features_to_stack.append(features_for_modality[:max_epochs, :])\n",
    "            else:\n",
    "                subject_features_to_stack.append(features_for_modality)\n",
    "\n",
    "            if subject_labels is None: # Only set labels if not already set by a previous modality\n",
    "                if labels_for_modality.shape[0] < max_epochs:\n",
    "                    padded_labels = np.zeros(max_epochs, dtype=int) # Assuming 0 is a valid padding for labels\n",
    "                    padded_labels[:labels_for_modality.shape[0]] = labels_for_modality\n",
    "                    subject_labels = padded_labels\n",
    "                elif labels_for_modality.shape[0] > max_epochs:\n",
    "                    subject_labels = labels_for_modality[:max_epochs]\n",
    "                else:\n",
    "                    subject_labels = labels_for_modality\n",
    "            # Optional: Add an else block here to check for label consistency if labels already set\n",
    "            elif not np.array_equal(subject_labels, labels_for_modality[:max_epochs]): # Compare truncated labels\n",
    "                print(f\"  Warning: Labels for subject {subject_id} differ between modalities. Using the first available labels.\")\n",
    "        else:\n",
    "            print(f\"  Warning: {modality.upper()} data missing or empty for subject {subject_id}. Appending zeros.\")\n",
    "            subject_features_to_stack.append(np.zeros((max_epochs, len(feat_names_list))))\n",
    "            if subject_labels is None: # Only set labels if not already set by a previous modality\n",
    "                subject_labels = np.zeros(max_epochs, dtype=int) # Default label for missing epochs\n",
    "\n",
    "    # Final combination for the subject\n",
    "    if not subject_features_to_stack or (len(subject_features_to_stack) > 0 and sum(f.shape[1] for f in subject_features_to_stack) == 0):\n",
    "        print(f\"Skipping subject {subject_id}: No valid features to combine across modalities.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        combined_features = np.hstack(subject_features_to_stack)\n",
    "        if subject_labels is None:\n",
    "            raise ValueError(\"Labels could not be determined for subject after combining features.\")\n",
    "\n",
    "        combined_all_features_by_subject[subject_id] = combined_features\n",
    "        combined_all_labels_by_subject[subject_id] = subject_labels\n",
    "        print(f\"  Subject {subject_id}: Combined features shape {combined_features.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error combining features for subject {subject_id}: {e}. This usually means feature arrays have inconsistent numbers of samples (rows).\")\n",
    "        print(f\"  Shapes of features to stack: {[f.shape for f in subject_features_to_stack]}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"🔄 INTER-SUBJECT CLASSIFICATION (Combined COH+DPLI+MI+pearson_r Features - LOSO)\")\n",
    "print(f\"==================================================\")\n",
    "\n",
    "loso_accuracies_summary = {name: [] for name in models_and_params}\n",
    "loso_f1_summary = {name: [] for name in models_and_params}\n",
    "loso_roc_auc_summary = {name: [] for name in models_and_params}\n",
    "\n",
    "# Store FPRs and TPRs for plotting average ROC curves\n",
    "all_fprs = defaultdict(list)\n",
    "all_tprs = defaultdict(list)\n",
    "# all_aucs = defaultdict(list) # This is already stored in loso_roc_auc_summary\n",
    "\n",
    "actual_subjects_with_combined_data = list(combined_all_features_by_subject.keys())\n",
    "\n",
    "if not actual_subjects_with_combined_data:\n",
    "    print(\"No subjects with valid combined data for LOSO. Exiting classification.\")\n",
    "    # If no subjects, the rest of the script will print summaries based on empty lists.\n",
    "\n",
    "for test_subject in actual_subjects_with_combined_data:\n",
    "    X_test_raw = combined_all_features_by_subject[test_subject]\n",
    "    y_test = combined_all_labels_by_subject[test_subject]\n",
    "\n",
    "    X_train_list = []\n",
    "    y_train_list = []\n",
    "    for s in actual_subjects_with_combined_data:\n",
    "        if s != test_subject:\n",
    "            X_train_list.append(combined_all_features_by_subject[s])\n",
    "            y_train_list.append(combined_all_labels_by_subject[s])\n",
    "    \n",
    "    if not X_train_list:\n",
    "        print(f\"Skipping classification for {test_subject}: No training data available.\")\n",
    "        for name in models_and_params:\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "        continue\n",
    "\n",
    "    X_train_raw = np.vstack(X_train_list)\n",
    "    y_train = np.hstack(y_train_list)\n",
    "\n",
    "    print(f\"\\n--- Testing on Subject {test_subject} (Combined COH+DPLI+MI Features) ---\")\n",
    "    print(f\"  Train samples: {X_train_raw.shape[0]}, Test samples: {X_test_raw.shape[0]}\")\n",
    "    print(f\"  Train features: {X_train_raw.shape[1]}, Test features: {X_test_raw.shape[1]}\")\n",
    "\n",
    "    if X_train_raw.shape[0] == 0 or X_test_raw.shape[0] == 0:\n",
    "        print(f\"Skipping classification for {test_subject}: Empty train or test set.\")\n",
    "        for name in models_and_params:\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "        continue\n",
    "    \n",
    "    if X_train_raw.shape[1] == 0:\n",
    "        print(f\"Skipping classification for {test_subject}: No features available after initial combining (all zeros/empty).\")\n",
    "        for name in models_and_params:\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "        continue\n",
    "\n",
    "    # --- Start of Proper Preprocessing Pipeline within LOSO Fold ---\n",
    "    \n",
    "    # Imputation (fit on train, transform on both)\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    X_train_imputed = imputer.fit_transform(X_train_raw)\n",
    "    X_test_imputed = imputer.transform(X_test_raw)\n",
    "\n",
    "    # Z-score Normalization (fit on train, transform on both)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "    # Variance Thresholding (fit on train, transform on both)\n",
    "    var_thresh = VarianceThreshold(threshold=1e-5)\n",
    "    X_train_f = var_thresh.fit_transform(X_train_scaled)\n",
    "    X_test_f = var_thresh.transform(X_test_scaled)\n",
    "    original_indices_after_var_thresh = var_thresh.get_support(indices=True)\n",
    "    \n",
    "    if X_train_f.shape[1] == 0:\n",
    "        print(f\"  No features remaining after variance threshold for {test_subject}. Skipping models.\")\n",
    "        for name in models_and_params:\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "        continue\n",
    "\n",
    "    for name, (model, param_grid) in models_and_params.items():\n",
    "        # Apply t-test feature selection (fit on train, transform on both)\n",
    "        X_train_sel, X_test_sel, selected_relative_indices = ttest_feature_selection(X_train_f, y_train, X_test_f)\n",
    "\n",
    "        # Map selected indices back to the original combined feature names for printing\n",
    "        selected_feature_names = np.array([])\n",
    "        if len(selected_relative_indices) > 0 and len(original_indices_after_var_thresh) > 0:\n",
    "            selected_global_indices = original_indices_after_var_thresh[selected_relative_indices]\n",
    "            selected_feature_names = np.array(combined_all_feature_names)[selected_global_indices]\n",
    "\n",
    "        print(f\"  Model: {name}\")\n",
    "        print(f\"    Selected {len(selected_feature_names)} features out of {X_train_raw.shape[1]} original combined features.\")\n",
    "        if len(selected_feature_names) > 0:\n",
    "            print(f\"    Selected Features: {', '.join(selected_feature_names[:10])}{'...' if len(selected_feature_names) > 10 else ''}\") # Print only first 10 for brevity\n",
    "        else:\n",
    "            print(\"    No features selected.\")\n",
    "\n",
    "        if X_train_sel.shape[1] == 0:\n",
    "            print(f\"    No features remaining after t-test selection. Skipping classification for this model.\")\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "            continue\n",
    "        \n",
    "        unique_classes_train = np.unique(y_train)\n",
    "        if X_train_sel.shape[0] < 3 or len(unique_classes_train) < 2:\n",
    "            print(f\"    Not enough samples ({X_train_sel.shape[0]}) or classes ({len(unique_classes_train)}) in training set for RandomizedSearchCV. Skipping classification.\")\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "            continue\n",
    "\n",
    "        # Grid search for best hyperparameters\n",
    "        try:\n",
    "            if param_grid:\n",
    "                # Using RandomizedSearchCV for faster tuning\n",
    "                # n_iter: Number of parameter settings that are sampled. Reduce for faster runs.\n",
    "                clf = RandomizedSearchCV(model, param_grid, n_iter=min(20, len(list(model.get_params().keys())) * 2), cv=3, scoring='accuracy', n_jobs=-1, verbose=0, random_state=42) # Adjust n_iter based on param grid size\n",
    "                clf.fit(X_train_sel, y_train)\n",
    "                best_estimator = clf.best_estimator_\n",
    "                best_params = clf.best_params_\n",
    "            else: # For models like GaussianNB with no parameters to tune\n",
    "                best_estimator = model\n",
    "                best_estimator.fit(X_train_sel, y_train)\n",
    "                best_params = 'N/A'\n",
    "\n",
    "            y_pred = best_estimator.predict(X_test_sel)\n",
    "            acc = accuracy_score(y_test, y_pred) * 100\n",
    "            f1 = f1_score(y_test, y_pred) # Calculate F1-score\n",
    "\n",
    "            loso_accuracies_summary[name].append(acc)\n",
    "            loso_f1_summary[name].append(f1) # Store F1-score\n",
    "\n",
    "            print(f\"    Accuracy: {acc:.2f}%\")\n",
    "            print(f\"    F1-score: {f1:.2f}\") # Print F1-score\n",
    "            print(f\"    Best Params: {best_params}\")\n",
    "            if np.unique(y_test).size == 2 and y_test.shape[0] >= 2:\n",
    "                print(f\"    Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "                # Calculate ROC AUC and store FPR/TPR for plotting\n",
    "                if hasattr(best_estimator, \"predict_proba\"):\n",
    "                    y_prob = best_estimator.predict_proba(X_test_sel)[:, 1]\n",
    "                    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "                    loso_roc_auc_summary[name].append(roc_auc)\n",
    "                    print(f\"    ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "                    # Store FPR/TPR for average ROC curve\n",
    "                    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "                    all_fprs[name].append(fpr)\n",
    "                    all_tprs[name].append(tpr)\n",
    "                else:\n",
    "                    loso_roc_auc_summary[name].append(np.nan)\n",
    "                    print(\"    Model does not support predict_proba, skipping ROC AUC.\")\n",
    "            else:\n",
    "                loso_roc_auc_summary[name].append(np.nan)\n",
    "                print(\"    Cannot compute confusion matrix or ROC (not enough unique classes or samples in test set).\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Error during classification for {name}: {e}. Skipping.\")\n",
    "            loso_accuracies_summary[name].append(np.nan)\n",
    "            loso_f1_summary[name].append(np.nan)\n",
    "            loso_roc_auc_summary[name].append(np.nan)\n",
    "            continue\n",
    "\n",
    "# Store the summary results for the combined features\n",
    "analysis_results['combined_coh_dpli_mi_pearson_r'] = { # Changed key to include pearson_r\n",
    "    'loso_accuracies_summary': loso_accuracies_summary,\n",
    "    'loso_f1_summary': loso_f1_summary,\n",
    "    'loso_roc_auc_summary': loso_roc_auc_summary\n",
    "}\n",
    "\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"📊 OVERALL LOSO ACCURACY, F1-SCORE, and ROC AUC SUMMARY (Combined COH+DPLI+MI+pearson_r)\")\n",
    "print(f\"==================================================\")\n",
    "for name, accs in loso_accuracies_summary.items():\n",
    "    valid_accs = [a for a in accs if not np.isnan(a)]\n",
    "    valid_f1s = [f for f in loso_f1_summary[name] if not np.isnan(f)]\n",
    "    valid_aucs = [a for a in loso_roc_auc_summary[name] if not np.isnan(a)]\n",
    "\n",
    "    if valid_accs:\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Accuracy: {np.mean(valid_accs):.2f}% ± {np.std(valid_accs):.2f}%\")\n",
    "        if valid_f1s:\n",
    "            print(f\"  F1-score: {np.mean(valid_f1s):.2f} ± {np.std(valid_f1s):.2f}\")\n",
    "        else:\n",
    "            print(f\"  F1-score: No valid F1-scores to report.\")\n",
    "        if valid_aucs:\n",
    "            print(f\"  ROC AUC: {np.mean(valid_aucs):.2f} ± {np.std(valid_aucs):.2f}\")\n",
    "        else:\n",
    "            print(f\"  ROC AUC: No valid ROC AUCs to report.\")\n",
    "    else:\n",
    "        print(f\"{name}: No valid metrics to report.\")\n",
    "\n",
    "print(\"\\n--- Finished classification for combined connectivity types ---\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n==================================================\")\n",
    "print(f\"📈 AVERAGE ROC CURVES (Combined COH+DPLI+MI+pearson_r Features)\")\n",
    "print(f\"==================================================\")\n",
    "\n",
    "# Create the main ROC curve plot\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(8, 5))\n",
    "ax_roc.set_title('Average ROC Curve for Each Model (LOSO Cross-Validation)')\n",
    "ax_roc.set_xlabel('False Positive Rate')\n",
    "ax_roc.set_ylabel('True Positive Rate')\n",
    "ax_roc.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Collect handles and labels for the legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "# Add the 'Chance' line to the legend handles and labels\n",
    "handles.append(ax_roc.lines[-1]) # Get the last line added, which is 'Chance'\n",
    "labels.append('Chance')\n",
    "\n",
    "for name in models_and_params:\n",
    "    if all_fprs[name]: # Check if there is any ROC data for the model\n",
    "        tprs_interp = []\n",
    "        aucs_for_plot_std = [] # Collect AUCs for std dev calculation in plot legend\n",
    "        \n",
    "        # Ensure we only iterate up to the minimum number of folds available for this model\n",
    "        min_folds = min(len(all_fprs[name]), len(all_tprs[name]))\n",
    "\n",
    "        for i in range(min_folds):\n",
    "            # Interpolate all ROC curves to the common mean_fpr\n",
    "            tprs_interp.append(np.interp(mean_fpr, all_fprs[name][i], all_tprs[name][i]))\n",
    "            tprs_interp[-1][0] = 0.0 # Ensure the curve starts at (0,0)\n",
    "\n",
    "            # Get the AUC for the current fold for std dev calculation\n",
    "            # Use the already stored AUCs from loso_roc_auc_summary for consistency\n",
    "            if not np.isnan(loso_roc_auc_summary[name][i]):\n",
    "                aucs_for_plot_std.append(loso_roc_auc_summary[name][i])\n",
    "\n",
    "\n",
    "        if tprs_interp: # Check if there are any interpolated TPRs\n",
    "            mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "            mean_tpr[-1] = 1.0 # Ensure the curve ends at (1,1)\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            \n",
    "            # Calculate standard deviation of AUCs if available (though not used in legend label anymore)\n",
    "            std_auc_val = np.std(aucs_for_plot_std) if aucs_for_plot_std else 0.0 # Default to 0 if no valid AUCs\n",
    "\n",
    "            line, = ax_roc.plot(mean_fpr, mean_tpr,\n",
    "                                # Removed std_auc_val from the label\n",
    "                                label=r'Mean %s ROC (AUC = %0.2f)' % (name, mean_auc),\n",
    "                                lw=2, alpha=.8)\n",
    "            handles.append(line)\n",
    "            # Removed std_auc_val from the label for the handles list as well\n",
    "            labels.append(r'Mean %s ROC (AUC = %0.2f)' % (name, mean_auc))\n",
    "\n",
    "\n",
    "            std_tpr = np.std(tprs_interp, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            # Removed the fill_between_plot from the legend\n",
    "            ax_roc.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No valid interpolated TPRs to plot for {name} after processing folds.\")\n",
    "    else:\n",
    "        print(f\"No ROC data collected for {name}. Skipping plotting for this model.\")\n",
    "\n",
    "ax_roc.grid(True)\n",
    "plt.show() # Display the main ROC plot\n",
    "\n",
    "# Create a separate plot for the legend\n",
    "fig_legend = plt.figure(figsize=(6, len(models_and_params) * 0.75 + 1)) # Adjust size based on number of models\n",
    "ax_legend = fig_legend.add_subplot(111)\n",
    "ax_legend.legend(handles, labels, loc='center', frameon=False, prop={'size': 10}) # Make text smaller\n",
    "ax_legend.axis('off') # Hide the axes\n",
    "plt.show() # Display the legend plot\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: loso_accuracies_summary must be defined already as a dictionary\n",
    "# loso_accuracies_summary = {'SVM': [...], 'MLP': [...], ...}\n",
    "\n",
    "# Remove Bernoulli Naive Bayes if present\n",
    "model_names = [model for model in loso_accuracies_summary.keys() if model != \"Bernoulli Naive Bayes\"]\n",
    "average_accuracies = [\n",
    "    np.nanmean(loso_accuracies_summary[model]) if len(loso_accuracies_summary[model]) > 0 else 0\n",
    "    for model in model_names\n",
    "]\n",
    "\n",
    "# Convert to numpy array\n",
    "average_accuracies = np.array(average_accuracies)\n",
    "\n",
    "# Radar plot setup\n",
    "num_vars = len(model_names)\n",
    "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
    "\n",
    "# Close the radar plot circle\n",
    "average_accuracies = np.concatenate((average_accuracies, [average_accuracies[0]]))\n",
    "angles += [angles[0]]\n",
    "\n",
    "# Colormap for unique colors per model\n",
    "colormap = plt.colormaps['tab10'] if num_vars <= 10 else plt.colormaps['hsv']\n",
    "colors = [colormap(i / num_vars) for i in range(num_vars)]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Plot lines and fill (single main line for average)\n",
    "ax.plot(angles, average_accuracies, color='black', linewidth=2, label='Accuracy Curve')\n",
    "ax.fill(angles, average_accuracies, color='lightgray', alpha=0.2)\n",
    "\n",
    "# Add each model point with a different color\n",
    "for i in range(num_vars):\n",
    "    ax.plot(angles[i], average_accuracies[i], 'o', color=colors[i], markersize=10)\n",
    "    ax.text(angles[i], average_accuracies[i] + 5, f'{average_accuracies[i]:.1f}%', \n",
    "            horizontalalignment='center', verticalalignment='bottom', fontsize=12, color=colors[i])\n",
    "\n",
    "# Axis labels and styling\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(model_names, fontsize=13, fontweight='bold')\n",
    "\n",
    "ax.set_title('Average LOSO Accuracy per Classifier', size=18, y=1.1, fontweight='bold')\n",
    "\n",
    "# Radial labels\n",
    "ax.set_rlabel_position(180 / num_vars)\n",
    "ax.set_yticks([70, 75, 80, 85, 90, 95, 100])\n",
    "ax.set_yticklabels([\"70%\", \"75%\", \"80%\", \"85%\", \"90%\", \"95%\", \"100%\"], fontsize=12)\n",
    "ax.set_ylim(50, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"model_accuracy_radar_colored_points.png\", dpi=600)\n",
    "\n",
    "print(\"\\n--- Plotting finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bc5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier,\n",
    "                              AdaBoostClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.stats import ttest_ind\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def ttest_feature_selection(X_train, y_train, X_test, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs feature selection using independent t-tests.\n",
    "    Selects features where the p-value is below the given alpha threshold.\n",
    "    Includes a fallback to select top 10 features if none pass the threshold.\n",
    "    \"\"\"\n",
    "    X_att = X_train[y_train == 1]\n",
    "    X_inatt = X_train[y_train == 0]\n",
    "\n",
    "    # Handle cases where one class might be empty or not enough samples for t-test\n",
    "    if X_att.shape[0] < 2 or X_inatt.shape[0] < 2:\n",
    "        print(\"Warning: Not enough samples in one or both classes for t-test. Skipping t-test based selection, returning all features.\")\n",
    "        if X_train.shape[1] > 0:\n",
    "            return X_train, X_test, np.arange(X_train.shape[1])\n",
    "        else: # No features to begin with\n",
    "            return np.array([]).reshape(X_train.shape[0], 0), \\\n",
    "                   np.array([]).reshape(X_test.shape[0], 0), \\\n",
    "                   np.array([])\n",
    "\n",
    "    if X_train.shape[1] == 0:\n",
    "        print(\"Warning: No features in input to t-test selection. Returning empty arrays.\")\n",
    "        return np.array([]).reshape(X_train.shape[0], 0), \\\n",
    "               np.array([]).reshape(X_test.shape[0], 0), \\\n",
    "               np.array([])\n",
    "\n",
    "    t_vals, p_vals = ttest_ind(X_att, X_inatt, axis=0, equal_var=False)\n",
    "    selected_indices = np.where(p_vals < alpha)[0]\n",
    "\n",
    "    # Fallback if no features pass threshold or if t-test was skipped\n",
    "    if len(selected_indices) == 0:\n",
    "        print(f\"Warning: No features passed t-test threshold (alpha={alpha}). Selecting top 10 features by p-value.\")\n",
    "        if p_vals.size > 0: # Ensure p_vals is not empty before sorting\n",
    "            selected_indices = np.argsort(p_vals)[:min(10, p_vals.size)] # Select top 10 or all if less than 10\n",
    "        else: # If no p-values (e.g., no features to begin with)\n",
    "            selected_indices = np.array([])\n",
    "        \n",
    "        # Final fallback to select all features if nothing else worked and features exist\n",
    "        if len(selected_indices) == 0 and X_train.shape[1] > 0:\n",
    "            selected_indices = np.arange(X_train.shape[1])\n",
    "\n",
    "    if X_train.shape[1] == 0 or len(selected_indices) == 0:\n",
    "        print(\"Warning: No features left after t-test selection. Returning empty arrays.\")\n",
    "        return np.array([]).reshape(X_train.shape[0], 0), \\\n",
    "               np.array([]).reshape(X_test.shape[0], 0), \\\n",
    "               np.array([])\n",
    "\n",
    "    return X_train[:, selected_indices], X_test[:, selected_indices], selected_indices\n",
    "\n",
    "# --- Models and hyperparameter grids (Optimized for speed) ---\n",
    "models_and_params = {\n",
    "    'LogisticRegression': (LogisticRegression(max_iter=2000, random_state=42), {\n",
    "        'C': [0.001, 0.01, 0.1, 1], # Reduced range\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'solver': ['saga'], # Focus on saga for elasticnet\n",
    "        'l1_ratio': [0.1, 0.5, 0.9], # For elasticnet\n",
    "        'max_iter': [1000] # Single value, good enough if convergence is met\n",
    "    }),\n",
    "    \"SGD Classifier\": (SGDClassifier(random_state=42), {\n",
    "        'loss': ['hinge', 'log_loss', 'modified_huber'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'alpha': [0.0001, 0.001, 0.01], # Reduced range\n",
    "        'l1_ratio': [0.3, 0.7],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'eta0': [0.01, 0.1]\n",
    "    }),\n",
    "    'MLP': (MLPClassifier(max_iter=2000, random_state=42), {\n",
    "        'hidden_layer_sizes': [\n",
    "            (100,), (150,), # Single layer\n",
    "            (100, 50) # Two layers\n",
    "        ],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'solver': ['adam'], # Adam is generally faster\n",
    "        'alpha': [0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'learning_rate_init': [0.0001, 0.001],\n",
    "    }),\n",
    "    'RandomForest': (RandomForestClassifier(random_state=42), {\n",
    "        'n_estimators': [100, 200], # Fewer options\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4],\n",
    "        'max_features': ['sqrt', None], # Simplified options\n",
    "        'bootstrap': [True],\n",
    "        'criterion': ['gini']\n",
    "    }),\n",
    "    \"Extra Trees\": (ExtraTreesClassifier(random_state=42), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 20],\n",
    "        'min_samples_split': [2, 10],\n",
    "        'min_samples_leaf': [1, 4],\n",
    "        'max_features': ['sqrt', None],\n",
    "        'criterion': ['gini']\n",
    "    }),\n",
    "    \"Gradient Boosting\": (GradientBoostingClassifier(random_state=42), {\n",
    "        'n_estimators': [100], # Fewer options\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'min_samples_split': [2],\n",
    "        'min_samples_leaf': [1],\n",
    "        'max_features': ['sqrt']\n",
    "    }),\n",
    "    \"XGBoost\": (XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42, tree_method='hist'), {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 6],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'gamma': [0, 0.2],\n",
    "        'reg_alpha': [0, 0.1],\n",
    "        'reg_lambda': [0.1, 1]\n",
    "    }),\n",
    "    'SVM': (SVC(probability=True, random_state=42), {\n",
    "        'C': [0.1, 1], # Reduced range\n",
    "        'kernel': ['rbf'], # Focus on most common kernel\n",
    "        'gamma': ['scale', 0.1],\n",
    "        'shrinking': [True],\n",
    "        'class_weight': [None] # Reduced option\n",
    "    }),\n",
    "    \"Gaussian Naive Bayes\": (GaussianNB(), {\n",
    "        'var_smoothing': [1e-9, 1e-7] # Reduced options\n",
    "    }),\n",
    "    \"Bernoulli Naive Bayes\": (BernoulliNB(), {\n",
    "        'alpha': [0.1, 1.0],\n",
    "        'binarize': [0.0], # A typical binarization threshold\n",
    "        'fit_prior': [True, False]\n",
    "    })\n",
    "}\n",
    "\n",
    "print(\"\\n--- Combining COH, DPLI, pearson_r and MI features and names for each subject ---\")\n",
    "combined_all_features_by_subject = {}\n",
    "combined_all_labels_by_subject = {}\n",
    "\n",
    "all_subject_ids_sets = []\n",
    "for modality in ['coh', 'dpli', 'mi', 'pearson_r']:\n",
    "    if modality in analysis_results and 'combined_features_by_subject' in analysis_results[modality]:\n",
    "        all_subject_ids_sets.append(set(analysis_results[modality]['combined_features_by_subject'].keys()))\n",
    "\n",
    "if not all_subject_ids_sets:\n",
    "    print(\"Error: No connectivity results found in 'analysis_results'. Please ensure previous steps ran successfully.\")\n",
    "    # If analysis_results is truly empty, the rest of the script will likely fail gracefully with warnings/empty results.\n",
    "    # For a runnable script, we'll proceed assuming dummy data is enough.\n",
    "\n",
    "subject_ids = sorted(list(set.union(*all_subject_ids_sets)))\n",
    "\n",
    "# Get feature names for COH, DPLI, MI, and pearson_r\n",
    "coh_feature_names = analysis_results.get('coh', {}).get('all_feature_names', [])\n",
    "dpli_feature_names = analysis_results.get('dpli', {}).get('all_feature_names', [])\n",
    "mi_feature_names = analysis_results.get('mi', {}).get('all_feature_names', [])\n",
    "pearson_r_feature_names = analysis_results.get('pearson_r', {}).get('all_feature_names', [])\n",
    "\n",
    "# Create the combined feature names list\n",
    "combined_all_feature_names = coh_feature_names + dpli_feature_names + mi_feature_names + pearson_r_feature_names\n",
    "\n",
    "# Determine max_epochs for padding zero arrays if any modality is missing for a subject.\n",
    "max_epochs = 0\n",
    "for sub_id in subject_ids:\n",
    "    for modality in ['coh', 'dpli', 'mi', 'pearson_r']:\n",
    "        if modality in analysis_results and sub_id in analysis_results[modality].get('combined_features_by_subject', {}):\n",
    "            num_epochs = analysis_results[modality]['combined_features_by_subject'][sub_id].shape[0]\n",
    "            if num_epochs > max_epochs:\n",
    "                max_epochs = num_epochs\n",
    "\n",
    "if max_epochs == 0:\n",
    "    print(\"Warning: No epochs found across any subject or modality. Classification might fail.\")\n",
    "    # If no epochs, padding with zeros might be problematic or lead to empty arrays later.\n",
    "    # For robust demonstration, let's ensure max_epochs is at least 1 if there are any features.\n",
    "    if any(len(feat_names_list) > 0 for feat_names_list in [coh_feature_names, dpli_feature_names, mi_feature_names, pearson_r_feature_names]):\n",
    "        print(\"Forcing max_epochs to 1 as no epochs found but feature names exist.\")\n",
    "        max_epochs = 1 # Minimal epoch for the sake of array creation if truly no data.\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    print(f\"Processing subject {subject_id} for combined features...\")\n",
    "    subject_features_to_stack = []\n",
    "    subject_labels = None # Labels should ideally be consistent across modalities for a subject\n",
    "\n",
    "    for modality, feat_names_list in zip(['coh', 'dpli', 'mi', 'pearson_r'],\n",
    "                                         [coh_feature_names, dpli_feature_names, mi_feature_names, pearson_r_feature_names]):\n",
    "        \n",
    "        features_for_modality = analysis_results.get(modality, {}).get('combined_features_by_subject', {}).get(subject_id)\n",
    "        labels_for_modality = analysis_results.get(modality, {}).get('combined_labels_by_subject', {}).get(subject_id)\n",
    "\n",
    "        if features_for_modality is not None and features_for_modality.shape[0] > 0:\n",
    "            # Ensure features have correct number of columns\n",
    "            if features_for_modality.shape[1] != len(feat_names_list):\n",
    "                print(f\"  Warning: Feature count mismatch for {modality} of {subject_id}. Expected {len(feat_names_list)}, got {features_for_modality.shape[1]}. Skipping or padding might occur.\")\n",
    "                # Attempt to pad/truncate if mismatch, or raise error\n",
    "                # For robustness, we'll try to align to the expected number of features\n",
    "                if features_for_modality.shape[1] > len(feat_names_list):\n",
    "                    features_for_modality = features_for_modality[:, :len(feat_names_list)]\n",
    "                else: # Pad with zeros if fewer features than expected\n",
    "                    temp_features = np.zeros((features_for_modality.shape[0], len(feat_names_list)))\n",
    "                    temp_features[:, :features_for_modality.shape[1]] = features_for_modality\n",
    "                    features_for_modality = temp_features\n",
    "\n",
    "            # Pad or truncate features to max_epochs for consistent stacking\n",
    "            if features_for_modality.shape[0] < max_epochs:\n",
    "                padded_features = np.zeros((max_epochs, features_for_modality.shape[1]))\n",
    "                padded_features[:features_for_modality.shape[0], :] = features_for_modality\n",
    "                subject_features_to_stack.append(padded_features)\n",
    "            elif features_for_modality.shape[0] > max_epochs:\n",
    "                subject_features_to_stack.append(features_for_modality[:max_epochs, :])\n",
    "            else:\n",
    "                subject_features_to_stack.append(features_for_modality)\n",
    "\n",
    "            if subject_labels is None: # Only set labels if not already set by a previous modality\n",
    "                if labels_for_modality.shape[0] < max_epochs:\n",
    "                    padded_labels = np.zeros(max_epochs, dtype=int) # Assuming 0 is a valid padding for labels\n",
    "                    padded_labels[:labels_for_modality.shape[0]] = labels_for_modality\n",
    "                    subject_labels = padded_labels\n",
    "                elif labels_for_modality.shape[0] > max_epochs:\n",
    "                    subject_labels = labels_for_modality[:max_epochs]\n",
    "                else:\n",
    "                    subject_labels = labels_for_modality\n",
    "            # Optional: Add an else block here to check for label consistency if labels already set\n",
    "            elif not np.array_equal(subject_labels, labels_for_modality[:max_epochs]): # Compare truncated labels\n",
    "                print(f\"  Warning: Labels for subject {subject_id} differ between modalities. Using the first available labels.\")\n",
    "        else:\n",
    "            print(f\"  Warning: {modality.upper()} data missing or empty for subject {subject_id}. Appending zeros.\")\n",
    "            subject_features_to_stack.append(np.zeros((max_epochs, len(feat_names_list))))\n",
    "            if subject_labels is None: # Only set labels if not already set by a previous modality\n",
    "                subject_labels = np.zeros(max_epochs, dtype=int) # Default label for missing epochs\n",
    "\n",
    "    # Final combination for the subject\n",
    "    if not subject_features_to_stack or (len(subject_features_to_stack) > 0 and sum(f.shape[1] for f in subject_features_to_stack) == 0):\n",
    "        print(f\"Skipping subject {subject_id}: No valid features to combine across modalities.\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        combined_features = np.hstack(subject_features_to_stack)\n",
    "        if subject_labels is None:\n",
    "            raise ValueError(\"Labels could not be determined for subject after combining features.\")\n",
    "\n",
    "        combined_all_features_by_subject[subject_id] = combined_features\n",
    "        combined_all_labels_by_subject[subject_id] = subject_labels\n",
    "        print(f\"  Subject {subject_id}: Combined features shape {combined_features.shape}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error combining features for subject {subject_id}: {e}. This usually means feature arrays have inconsistent numbers of samples (rows).\")\n",
    "        print(f\"  Shapes of features to stack: {[f.shape for f in subject_features_to_stack]}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"🔄 SUBJECT-DEPENDENT CLASSIFICATION (Combined COH+DPLI+MI+pearson_r Features)\")\n",
    "print(f\"==================================================\")\n",
    "\n",
    "# Initialize dictionaries to store results for each subject\n",
    "subject_results = {}\n",
    "\n",
    "# Store FPRs and TPRs for plotting average ROC curves\n",
    "all_fprs = defaultdict(list)\n",
    "all_tprs = defaultdict(list)\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "n_folds = 2\n",
    "\n",
    "for subject_id in subject_ids:\n",
    "    print(f\"\\n--- Processing Subject {subject_id} ---\")\n",
    "    \n",
    "    # Get the data for this subject\n",
    "    X = combined_all_features_by_subject[subject_id]\n",
    "    y = combined_all_labels_by_subject[subject_id]\n",
    "    \n",
    "    # Skip if no data or not enough samples\n",
    "    if X.shape[0] == 0 or len(np.unique(y)) < 2:\n",
    "        print(f\"  Skipping subject {subject_id}: Not enough data or only one class present.\")\n",
    "        continue\n",
    "    \n",
    "    # Initialize dictionaries to store results for this subject\n",
    "    subject_results[subject_id] = {\n",
    "        'accuracies': {name: [] for name in models_and_params},\n",
    "        'f1_scores': {name: [] for name in models_and_params},\n",
    "        'roc_aucs': {name: [] for name in models_and_params}\n",
    "    }\n",
    "    \n",
    "    # Create cross-validation folds\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n",
    "        print(f\"\\n  Fold {fold_idx + 1}/{n_folds}\")\n",
    "        X_train_raw, X_test_raw = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "        \n",
    "        print(f\"    Train samples: {X_train_raw.shape[0]}, Test samples: {X_test_raw.shape[0]}\")\n",
    "        print(f\"    Features: {X_train_raw.shape[1]}\")\n",
    "        \n",
    "        if X_train_raw.shape[0] == 0 or X_test_raw.shape[0] == 0:\n",
    "            print(\"    Skipping fold: Empty train or test set.\")\n",
    "            continue\n",
    "            \n",
    "        if X_train_raw.shape[1] == 0:\n",
    "            print(\"    Skipping fold: No features available.\")\n",
    "            continue\n",
    "        \n",
    "        # --- Preprocessing Pipeline ---\n",
    "        # Imputation (fit on train, transform on both)\n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        X_train_imputed = imputer.fit_transform(X_train_raw)\n",
    "        X_test_imputed = imputer.transform(X_test_raw)\n",
    "\n",
    "        # Z-score Normalization (fit on train, transform on both)\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
    "        X_test_scaled = scaler.transform(X_test_imputed)\n",
    "\n",
    "        # Variance Thresholding (fit on train, transform on both)\n",
    "        var_thresh = VarianceThreshold(threshold=1e-5)\n",
    "        X_train_f = var_thresh.fit_transform(X_train_scaled)\n",
    "        X_test_f = var_thresh.transform(X_test_scaled)\n",
    "        original_indices_after_var_thresh = var_thresh.get_support(indices=True)\n",
    "        \n",
    "        if X_train_f.shape[1] == 0:\n",
    "            print(\"    No features remaining after variance threshold. Skipping models.\")\n",
    "            continue\n",
    "\n",
    "        for name, (model, param_grid) in models_and_params.items():\n",
    "            print(f\"    Model: {name}\")\n",
    "            \n",
    "            # Apply t-test feature selection (fit on train, transform on both)\n",
    "            X_train_sel, X_test_sel, selected_relative_indices = ttest_feature_selection(X_train_f, y_train, X_test_f)\n",
    "\n",
    "            # Map selected indices back to the original combined feature names for printing\n",
    "            selected_feature_names = np.array([])\n",
    "            if len(selected_relative_indices) > 0 and len(original_indices_after_var_thresh) > 0:\n",
    "                selected_global_indices = original_indices_after_var_thresh[selected_relative_indices]\n",
    "                selected_feature_names = np.array(combined_all_feature_names)[selected_global_indices]\n",
    "\n",
    "            print(f\"      Selected {len(selected_feature_names)} features out of {X_train_raw.shape[1]} original combined features.\")\n",
    "            if len(selected_feature_names) > 0:\n",
    "                print(f\"      Selected Features: {', '.join(selected_feature_names[:10])}{'...' if len(selected_feature_names) > 10 else ''}\")\n",
    "            else:\n",
    "                print(\"      No features selected.\")\n",
    "\n",
    "            if X_train_sel.shape[1] == 0:\n",
    "                print(\"      No features remaining after t-test selection. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            unique_classes_train = np.unique(y_train)\n",
    "            if X_train_sel.shape[0] < 3 or len(unique_classes_train) < 2:\n",
    "                print(\"      Not enough samples or classes in training set. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Grid search for best hyperparameters\n",
    "            try:\n",
    "                if param_grid:\n",
    "                    # Using RandomizedSearchCV for faster tuning\n",
    "                    clf = RandomizedSearchCV(model, param_grid, n_iter=min(20, len(list(model.get_params().keys())) * 2), \n",
    "                                           cv=2, scoring='accuracy', n_jobs=-1, verbose=0, random_state=42)\n",
    "                    clf.fit(X_train_sel, y_train)\n",
    "                    best_estimator = clf.best_estimator_\n",
    "                    best_params = clf.best_params_\n",
    "                else: # For models with no parameters to tune\n",
    "                    best_estimator = model\n",
    "                    best_estimator.fit(X_train_sel, y_train)\n",
    "                    best_params = 'N/A'\n",
    "\n",
    "                y_pred = best_estimator.predict(X_test_sel)\n",
    "                acc = accuracy_score(y_test, y_pred) * 100\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "                # Store results for this fold\n",
    "                subject_results[subject_id]['accuracies'][name].append(acc)\n",
    "                subject_results[subject_id]['f1_scores'][name].append(f1)\n",
    "\n",
    "                print(f\"      Accuracy: {acc:.2f}%\")\n",
    "                print(f\"      F1-score: {f1:.2f}\")\n",
    "                print(f\"      Best Params: {best_params}\")\n",
    "                if np.unique(y_test).size == 2 and y_test.shape[0] >= 2:\n",
    "                    print(f\"      Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "                    # Calculate ROC AUC and store FPR/TPR for plotting\n",
    "                    if hasattr(best_estimator, \"predict_proba\"):\n",
    "                        y_prob = best_estimator.predict_proba(X_test_sel)[:, 1]\n",
    "                        roc_auc = roc_auc_score(y_test, y_prob)\n",
    "                        subject_results[subject_id]['roc_aucs'][name].append(roc_auc)\n",
    "                        print(f\"      ROC AUC: {roc_auc:.2f}\")\n",
    "\n",
    "                        # Store FPR/TPR for average ROC curve\n",
    "                        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "                        all_fprs[name].append(fpr)\n",
    "                        all_tprs[name].append(tpr)\n",
    "                    else:\n",
    "                        subject_results[subject_id]['roc_aucs'][name].append(np.nan)\n",
    "                        print(\"      Model does not support predict_proba, skipping ROC AUC.\")\n",
    "                else:\n",
    "                    subject_results[subject_id]['roc_aucs'][name].append(np.nan)\n",
    "                    print(\"      Cannot compute confusion matrix or ROC (not enough unique classes or samples in test set).\")\n",
    "            except Exception as e:\n",
    "                print(f\"      Error during classification: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "# Store the summary results for the combined features\n",
    "analysis_results['combined_coh_dpli_mi_pearson_r_subject_dependent'] = {\n",
    "    'subject_results': subject_results\n",
    "}\n",
    "\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"📊 SUBJECT-DEPENDENT CLASSIFICATION SUMMARY\")\n",
    "print(f\"==================================================\")\n",
    "\n",
    "# Calculate and print average performance across subjects\n",
    "for name in models_and_params:\n",
    "    print(f\"\\n{name}:\")\n",
    "    \n",
    "    all_accs = []\n",
    "    all_f1s = []\n",
    "    all_aucs = []\n",
    "    \n",
    "    for subject_id in subject_ids:\n",
    "        if subject_id in subject_results:\n",
    "            accs = [a for a in subject_results[subject_id]['accuracies'][name] if not np.isnan(a)]\n",
    "            f1s = [f for f in subject_results[subject_id]['f1_scores'][name] if not np.isnan(f)]\n",
    "            aucs = [a for a in subject_results[subject_id]['roc_aucs'][name] if not np.isnan(a)]\n",
    "            \n",
    "            if accs:\n",
    "                subj_mean_acc = np.mean(accs)\n",
    "                all_accs.append(subj_mean_acc)\n",
    "            if f1s:\n",
    "                subj_mean_f1 = np.mean(f1s)\n",
    "                all_f1s.append(subj_mean_f1)\n",
    "            if aucs:\n",
    "                subj_mean_auc = np.mean(aucs)\n",
    "                all_aucs.append(subj_mean_auc)\n",
    "    \n",
    "    if all_accs:\n",
    "        print(f\"  Accuracy across subjects: {np.mean(all_accs):.2f}% ± {np.std(all_accs):.2f}%\")\n",
    "    else:\n",
    "        print(\"  No valid accuracy scores to report.\")\n",
    "    \n",
    "    if all_f1s:\n",
    "        print(f\"  F1-score across subjects: {np.mean(all_f1s):.2f} ± {np.std(all_f1s):.2f}\")\n",
    "    else:\n",
    "        print(\"  No valid F1-scores to report.\")\n",
    "    \n",
    "    if all_aucs:\n",
    "        print(f\"  ROC AUC across subjects: {np.mean(all_aucs):.2f} ± {np.std(all_aucs):.2f}\")\n",
    "    else:\n",
    "        print(\"  No valid ROC AUCs to report.\")\n",
    "\n",
    "print(\"\\n--- Finished subject-dependent classification ---\")\n",
    "\n",
    "# Plot ROC curves\n",
    "print(f\"\\n==================================================\")\n",
    "print(f\"📈 AVERAGE ROC CURVES (Subject-Dependent Classification)\")\n",
    "print(f\"==================================================\")\n",
    "\n",
    "# Create the main ROC curve plot\n",
    "fig_roc, ax_roc = plt.subplots(figsize=(8, 5))\n",
    "ax_roc.set_title('Average ROC Curve for Each Model (Subject-Dependent)')\n",
    "ax_roc.set_xlabel('False Positive Rate')\n",
    "ax_roc.set_ylabel('True Positive Rate')\n",
    "ax_roc.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Chance', alpha=.8)\n",
    "\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "# Collect handles and labels for the legend\n",
    "handles = []\n",
    "labels = []\n",
    "\n",
    "# Add the 'Chance' line to the legend handles and labels\n",
    "handles.append(ax_roc.lines[-1]) # Get the last line added, which is 'Chance'\n",
    "labels.append('Chance')\n",
    "\n",
    "for name in models_and_params:\n",
    "    if all_fprs[name]: # Check if there is any ROC data for the model\n",
    "        tprs_interp = []\n",
    "        aucs_for_plot_std = [] # Collect AUCs for std dev calculation in plot legend\n",
    "        \n",
    "        # Ensure we only iterate up to the minimum number of folds available for this model\n",
    "        min_folds = min(len(all_fprs[name]), len(all_tprs[name]))\n",
    "\n",
    "        for i in range(min_folds):\n",
    "            # Interpolate all ROC curves to the common mean_fpr\n",
    "            tprs_interp.append(np.interp(mean_fpr, all_fprs[name][i], all_tprs[name][i]))\n",
    "            tprs_interp[-1][0] = 0.0 # Ensure the curve starts at (0,0)\n",
    "\n",
    "            # Get the AUC for the current fold for std dev calculation\n",
    "            # Use the already stored AUCs from subject_results for consistency\n",
    "            # This part would need to be adapted based on how you store AUCs in subject_results\n",
    "\n",
    "        if tprs_interp: # Check if there are any interpolated TPRs\n",
    "            mean_tpr = np.mean(tprs_interp, axis=0)\n",
    "            mean_tpr[-1] = 1.0 # Ensure the curve ends at (1,1)\n",
    "            mean_auc = auc(mean_fpr, mean_tpr)\n",
    "            \n",
    "            line, = ax_roc.plot(mean_fpr, mean_tpr,\n",
    "                              label=r'Mean %s ROC (AUC = %0.2f)' % (name, mean_auc),\n",
    "                              lw=2, alpha=.8)\n",
    "            handles.append(line)\n",
    "            labels.append(r'Mean %s ROC (AUC = %0.2f)' % (name, mean_auc))\n",
    "\n",
    "            std_tpr = np.std(tprs_interp, axis=0)\n",
    "            tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "            tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "            ax_roc.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2)\n",
    "            \n",
    "        else:\n",
    "            print(f\"No valid interpolated TPRs to plot for {name} after processing folds.\")\n",
    "    else:\n",
    "        print(f\"No ROC data collected for {name}. Skipping plotting for this model.\")\n",
    "\n",
    "ax_roc.grid(True)\n",
    "plt.show() # Display the main ROC plot\n",
    "\n",
    "# Create a separate plot for the legend\n",
    "fig_legend = plt.figure(figsize=(6, len(models_and_params) * 0.75 + 1)) # Adjust size based on number of models\n",
    "ax_legend = fig_legend.add_subplot(111)\n",
    "ax_legend.legend(handles, labels, loc='center', frameon=False, prop={'size': 10}) # Make text smaller\n",
    "ax_legend.axis('off') # Hide the axes\n",
    "plt.show() # Display the legend plot\n",
    "\n",
    "print(\"\\n--- Plotting finished ---\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
